{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "897aee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DL2_DIR = r\"../dl2_gamma\"\n",
    "PATTERNS = [\"gamma_*.h5\", \"proton_*.h5\"]  \n",
    "\n",
    "import os, glob, h5py, textwrap, inspect\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import astropy.units as u\n",
    "from astropy.table import Table\n",
    "\n",
    "from ctapipe.io import read_table\n",
    "\n",
    "# Optional: matplotlib quick plots\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8982b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 files\n",
      " - gamma_200_300E3GeV_20_20deg_testing_dl1_dl2.h5\n",
      " - gamma_200_300E3GeV_30_30deg_testing_dl1_dl2.h5\n",
      " - gamma_200_300E3GeV_40_40deg_testing_dl1_dl2.h5\n",
      " - gamma_200_300E3GeV_60_60deg_testing_dl1_dl2.h5\n",
      " - gamma_point_50_300E3GeV_20_20deg_testing_dl1_dl2.h5\n"
     ]
    }
   ],
   "source": [
    "files = []\n",
    "for pat in PATTERNS:\n",
    "    files.extend(sorted(glob.glob(str(Path(DL2_DIR) / pat))))\n",
    "print(f\"Found {len(files)} files\")\n",
    "for f in files[:5]:\n",
    "    print(\" -\", Path(f).name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42cf4712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting file: gamma_200_300E3GeV_20_20deg_testing_dl1_dl2.h5\n",
      "Found parameter tables:\n",
      " - /dl2/event/telescope/parameters/stereo\n"
     ]
    }
   ],
   "source": [
    "# List available DL2 parameter tables in the FIRST matching DL2 file, using your existing DL2_DIR and PATTERNS variables\n",
    "\n",
    "import glob, h5py\n",
    "from pathlib import Path\n",
    "\n",
    "# Use existing variables; fall back gracefully if PATTERNS not defined\n",
    "try:\n",
    "    patterns = PATTERNS\n",
    "except NameError:\n",
    "    patterns = [\"*.h5\"]\n",
    "\n",
    "files = []\n",
    "for pat in patterns:\n",
    "    files.extend(sorted(glob.glob(str(Path(DL2_DIR) / pat))))\n",
    "\n",
    "assert files, f\"No DL2 files found in {DL2_DIR} with patterns {patterns}\"\n",
    "\n",
    "first_file = files[0]\n",
    "print(\"Inspecting file:\", Path(first_file).name)\n",
    "\n",
    "def list_parameter_tables(h5file):\n",
    "    out = []\n",
    "    with h5py.File(h5file, \"r\") as h5:\n",
    "        base = \"/dl2/event/telescope/parameters\"\n",
    "        if base not in h5:\n",
    "            print(\"Not found:\", base)\n",
    "            return out\n",
    "        for name in h5[base].keys():\n",
    "            out.append(f\"{base}/{name}\")\n",
    "    return out\n",
    "\n",
    "candidates = list_parameter_tables(first_file)\n",
    "print(\"Found parameter tables:\")\n",
    "for p in candidates:\n",
    "    print(\" -\", p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "398cd32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 393483 | Columns: 35\n",
      "First 30 columns: ['obs_id', 'event_id', 'true_az_tel', 'true_alt_tel', 'HillasReconstructor_core_x', 'HillasReconstructor_core_y', 'HillasReconstructor_h_max', 'true_az', 'true_alt', 'true_energy', 'log_true_energy', 'true_core_x', 'true_core_y', 'true_h_first_int', 'true_x_max', 'true_shower_primary_id', 'true_camera_x', 'true_camera_y', 'min_true_energy_cut', 'log_reco_energy', 'reco_energy', 'gammaness', 'camera_frame_hillas_intensity_tel1', 'camera_frame_hillas_width_tel1', 'camera_frame_hillas_length_tel1', 'leakage_intensity_width_2_tel1', 'camera_frame_hillas_intensity_tel2', 'camera_frame_hillas_width_tel2', 'camera_frame_hillas_length_tel2', 'leakage_intensity_width_2_tel2']\n",
      "Missing expected columns: []\n",
      "Energy columns used: true='true_energy', reco='reco_energy'\n",
      "true_energy range [TeV]: 0.3180175721645355 → 299.97344970703125\n",
      "reco_energy range [TeV]: 0.6018515435939218 → 279.7540775007091\n"
     ]
    }
   ],
   "source": [
    "from ctapipe.io import read_table\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "\n",
    "table_path = \"/dl2/event/telescope/parameters/stereo\"\n",
    "tab = read_table(first_file, table_path)\n",
    "\n",
    "print(f\"Rows: {len(tab)} | Columns: {len(tab.colnames)}\")\n",
    "print(\"First 30 columns:\", tab.colnames[:30])\n",
    "\n",
    "expected = [\"true_energy\",\"reco_energy\",\"gammaness\",\"true_alt\",\"true_az\",\"reco_alt\",\"reco_az\"]\n",
    "missing = [c for c in expected if c not in tab.colnames]\n",
    "print(\"Missing expected columns:\", missing)\n",
    "\n",
    "def to_value_tev(col):\n",
    "    try:\n",
    "        return np.asarray(col.to_value(u.TeV), dtype=float)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return np.asarray(col.quantity.to_value(u.TeV), dtype=float)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return np.asarray(col, dtype=float)\n",
    "\n",
    "# try common fallbacks for energy names if needed\n",
    "true_name = \"true_energy\" if \"true_energy\" in tab.colnames else None\n",
    "reco_name = \"reco_energy\" if \"reco_energy\" in tab.colnames else None\n",
    "for alt in [\"mc_energy\",\"sim_energy_true\"]:\n",
    "    if true_name is None and alt in tab.colnames:\n",
    "        true_name = alt\n",
    "for alt in [\"energy\",\"reco_energy_mean\"]:\n",
    "    if reco_name is None and alt in tab.colnames:\n",
    "        reco_name = alt\n",
    "\n",
    "if true_name and reco_name:\n",
    "    te = to_value_tev(tab[true_name])\n",
    "    re = to_value_tev(tab[reco_name])\n",
    "    print(f\"Energy columns used: true='{true_name}', reco='{reco_name}'\")\n",
    "    print(\"true_energy range [TeV]:\", float(np.nanmin(te)), \"→\", float(np.nanmax(te)))\n",
    "    print(\"reco_energy range [TeV]:\", float(np.nanmin(re)), \"→\", float(np.nanmax(re)))\n",
    "else:\n",
    "    print(\"Could not resolve energy column names automatically.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20d900e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column presence: {'true_alt': True, 'true_az': True, 'reco_alt': True, 'reco_az': True, 'gammaness': True}\n",
      "theta_deg p50=9.079, p90=35.321\n",
      "gammaness percentiles: {50: 0.7627079732898246, 80: 0.8948992435384067, 90: 0.9347678921632108, 95: 0.9565131945032468, 99: 0.9802342503879063}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import astropy.units as u\n",
    "\n",
    "# confirm columns exist\n",
    "needed = [\"true_alt\",\"true_az\",\"reco_alt\",\"reco_az\",\"gammaness\"]\n",
    "present = {c: (c in tab.colnames) for c in needed}\n",
    "print(\"Column presence:\", present)\n",
    "\n",
    "def to_value_rad(col):\n",
    "    try:\n",
    "        return np.asarray(col.to_value(u.rad))\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return np.asarray(col.quantity.to_value(u.rad))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return np.asarray(col)\n",
    "\n",
    "def theta_from_altaz(true_alt, true_az, reco_alt, reco_az):\n",
    "    talt = to_value_rad(true_alt); taz = to_value_rad(true_az)\n",
    "    ralt = to_value_rad(reco_alt); raz = to_value_rad(reco_az)\n",
    "    cos_th = (np.sin(talt)*np.sin(ralt) + np.cos(talt)*np.cos(ralt)*np.cos(taz-raz))\n",
    "    cos_th = np.clip(cos_th, -1.0, 1.0)\n",
    "    return np.arccos(cos_th)\n",
    "\n",
    "if all(present[c] for c in [\"true_alt\",\"true_az\",\"reco_alt\",\"reco_az\"]):\n",
    "    theta_deg = np.rad2deg(theta_from_altaz(tab[\"true_alt\"], tab[\"true_az\"], tab[\"reco_alt\"], tab[\"reco_az\"]))\n",
    "    p50, p90 = np.nanpercentile(theta_deg, [50, 90])\n",
    "    print(f\"theta_deg p50={p50:.3f}, p90={p90:.3f}\")\n",
    "else:\n",
    "    print(\"Theta cannot be computed because one or more angle columns are missing.\")\n",
    "\n",
    "if present[\"gammaness\"]:\n",
    "    gh = np.asarray(tab[\"gammaness\"], dtype=float)\n",
    "    percs = {p: float(np.nanpercentile(gh, p)) for p in (50, 80, 90, 95, 99)}\n",
    "    print(\"gammaness percentiles:\", percs)\n",
    "else:\n",
    "    print(\"No 'gammaness' column to summarize.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9af7577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading metadata from: gamma_200_300E3GeV_20_20deg_testing_dl1_dl2.h5\n",
      "Has /simulation/service/shower_distribution: True\n",
      "Columns: ['obs_id', 'hist_id', 'n_entries', 'bins_energy', 'bins_core_dist', 'histogram']\n",
      "sum(n_entries) = 349860000\n",
      "energy bins TeV min/max ≈ 0.001 1000.0\n",
      "max core dist (m) ≈ 1760.0\n",
      "/simulation/run_config present: False\n",
      "/simulation/config present: False\n"
     ]
    }
   ],
   "source": [
    "from ctapipe.io import read_table\n",
    "import numpy as np\n",
    "\n",
    "def safe_read(path):\n",
    "    try:\n",
    "        return read_table(first_file, path)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "print(\"Reading metadata from:\", Path(first_file).name)\n",
    "\n",
    "sd = safe_read(\"/simulation/service/shower_distribution\")\n",
    "print(\"Has /simulation/service/shower_distribution:\", sd is not None)\n",
    "\n",
    "if sd is not None:\n",
    "    print(\"Columns:\", sd.colnames)\n",
    "    if \"n_entries\" in sd.colnames:\n",
    "        n_showers = int(np.nansum(np.asarray(sd[\"n_entries\"])))\n",
    "        print(\"sum(n_entries) =\", n_showers)\n",
    "    else:\n",
    "        n_showers = None\n",
    "        print(\"No 'n_entries' column found.\")\n",
    "    if \"bins_energy\" in sd.colnames:\n",
    "        eb = sd[\"bins_energy\"][0]\n",
    "        print(\"energy bins TeV min/max ≈\", float(np.min(eb)), float(np.max(eb)))\n",
    "    if \"bins_core_dist\" in sd.colnames:\n",
    "        cb = sd[\"bins_core_dist\"][0]\n",
    "        print(\"max core dist (m) ≈\", float(np.max(cb)))\n",
    "    if \"viewcone\" in sd.colnames:\n",
    "        try:\n",
    "            print(\"viewcone (deg) ≈\", float(sd[\"viewcone\"][0]))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "for alt in (\"/simulation/run_config\", \"/simulation/config\"):\n",
    "    t = safe_read(alt)\n",
    "    print(f\"{alt} present:\", t is not None)\n",
    "    if t is not None:\n",
    "        print(f\"{alt} columns:\", t.colnames[:20])\n",
    "        try:\n",
    "            print(t[:1])\n",
    "        except Exception:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e6ac27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aeff shape: (16,)\n",
      "Nonzero bins: 16 / 16\n",
      "First 10 Aeff(m^2): [ 1061580.25  2241499.92  3665457.77  5176082.75  6911585.7   9043213.99\n",
      " 11365037.89 14043049.95 17575390.66 21799004.04]\n",
      "Bin edges (TeV): [0.989 1.245 1.567 1.973 2.483 3.126] …\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from astropy.table import Table\n",
    "import inspect\n",
    "\n",
    "from pyirf.binning import create_bins_per_decade\n",
    "from pyirf.irf import effective_area_per_energy\n",
    "from pyirf.simulations import SimulatedEventsInfo\n",
    "\n",
    "# ---- build SimulatedEventsInfo, handling both API variants ----\n",
    "sig = inspect.signature(SimulatedEventsInfo)\n",
    "params = set(sig.parameters.keys())\n",
    "\n",
    "kwargs = dict(\n",
    "    n_showers=349_860_000,       # from sum(n_entries)\n",
    "    energy_min=0.001 * u.TeV,\n",
    "    energy_max=1000.0 * u.TeV,\n",
    "    max_impact=1760.0 * u.m,\n",
    "    spectral_index=-2.0,\n",
    ")\n",
    "\n",
    "if {\"viewcone_min\", \"viewcone_max\"} <= params:\n",
    "    kwargs.update(viewcone_min=0.0 * u.deg, viewcone_max=0.0 * u.deg)\n",
    "elif \"viewcone\" in params:\n",
    "    kwargs.update(viewcone=0.0 * u.deg)\n",
    "else:\n",
    "    print(\"Warning: SimulatedEventsInfo signature unexpected:\", params)\n",
    "\n",
    "sim_info = SimulatedEventsInfo(**kwargs)\n",
    "\n",
    "# ---- build bins from the actual data range to avoid empty bins ----\n",
    "def to_value_tev(col):\n",
    "    try:\n",
    "        return np.asarray(col.to_value(u.TeV), dtype=float)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return np.asarray(col.quantity.to_value(u.TeV), dtype=float)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return np.asarray(col, dtype=float)\n",
    "\n",
    "true_e = to_value_tev(tab[\"true_energy\"])\n",
    "emin = max(0.05, float(np.nanpercentile(true_e, 1)))\n",
    "emax = min(40.0, float(np.nanpercentile(true_e, 99.5)))\n",
    "true_bins = create_bins_per_decade((emin * u.TeV), (emax * u.TeV), bins_per_decade=10)\n",
    "\n",
    "tmini = Table()\n",
    "tmini[\"true_energy\"] = (true_e * u.TeV)\n",
    "\n",
    "aeff = effective_area_per_energy(\n",
    "    selected_events=tmini,\n",
    "    simulation_info=sim_info,\n",
    "    true_energy_bins=true_bins\n",
    ")\n",
    "\n",
    "vals = aeff.to_value(u.m**2)\n",
    "print(\"Aeff shape:\", vals.shape)\n",
    "print(\"Nonzero bins:\", int(np.count_nonzero(vals)), \"/\", len(vals))\n",
    "print(\"First 10 Aeff(m^2):\", np.round(vals[:10], 2))\n",
    "print(\"Bin edges (TeV):\", np.round(true_bins.to_value(u.TeV)[:6], 3), \"…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e55267d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-bin cuts computed: gh_eff=0.70 (upper-tail) → median cut=0.698; th_eff=0.70 → median cut=13.270 deg\n",
      "Selected events: 156866 / 393483  (39.9%)\n",
      "Bins with data: 16 / 16\n",
      "First 8 bins (Etrue_min→max TeV):\n",
      "  [0.989, 1.245)  N=  7940  kept=  4131  gh_cut=0.624  th_cut=19.266\n",
      "  [1.245, 1.567)  N= 13317  kept=  6944  gh_cut=0.664  th_cut=17.059\n",
      "  [1.567, 1.973)  N= 17298  kept=  9040  gh_cut=0.696  th_cut=15.188\n",
      "  [1.973, 2.483)  N= 19403  kept= 10144  gh_cut=0.717  th_cut=13.905\n",
      "  [2.483, 3.126)  N= 20580  kept= 10756  gh_cut=0.730  th_cut=13.008\n",
      "  [3.126, 3.936)  N= 21389  kept= 11316  gh_cut=0.733  th_cut=12.445\n",
      "  [3.936, 4.955)  N= 21352  kept= 11402  gh_cut=0.735  th_cut=11.947\n",
      "  [4.955, 6.238)  N= 20957  kept= 11303  gh_cut=0.731  th_cut=11.938\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import inspect\n",
    "import astropy.units as u\n",
    "from pyirf.cuts import calculate_percentile_cut, evaluate_binned_cut\n",
    "\n",
    "# use existing variables: tab, true_bins\n",
    "e_true_edges_TeV = true_bins.to_value(u.TeV)\n",
    "\n",
    "# choose efficiencies (keep these many events within each true-E bin)\n",
    "gh_eff = 0.70   # keep top 70% in gammaness (upper tail)\n",
    "th_eff = 0.70   # keep best 70% in theta (lower tail)\n",
    "\n",
    "def to_value_rad(col):\n",
    "    try:\n",
    "        return np.asarray(col.to_value(u.rad))\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return np.asarray(col.quantity.to_value(u.rad))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return np.asarray(col)\n",
    "\n",
    "def to_value_tev(col):\n",
    "    try:\n",
    "        return np.asarray(col.to_value(u.TeV), dtype=float)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return np.asarray(col.quantity.to_value(u.TeV), dtype=float)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return np.asarray(col, dtype=float)\n",
    "\n",
    "def theta_from_altaz(true_alt, true_az, reco_alt, reco_az):\n",
    "    talt = to_value_rad(true_alt); taz = to_value_rad(true_az)\n",
    "    ralt = to_value_rad(reco_alt); raz = to_value_rad(reco_az)\n",
    "    cos_th = (np.sin(talt)*np.sin(ralt) + np.cos(talt)*np.cos(ralt)*np.cos(taz-raz))\n",
    "    cos_th = np.clip(cos_th, -1.0, 1.0)\n",
    "    return np.arccos(cos_th)\n",
    "\n",
    "e_true = to_value_tev(tab[\"true_energy\"])\n",
    "gh = np.asarray(tab[\"gammaness\"], dtype=float)\n",
    "theta_deg = np.rad2deg(theta_from_altaz(tab[\"true_alt\"], tab[\"true_az\"], tab[\"reco_alt\"], tab[\"reco_az\"]))\n",
    "\n",
    "def safe_percentile_cut(values, bin_values, bins, eff_keep, keep_upper_tail=False):\n",
    "    # calculate_percentile_cut expects either 'efficiency' (0..1) or 'percentile' (0..100) of the *kept* side.\n",
    "    # For upper-tail keep (gammaness), we pass (1-eff) because pyirf computes '<= cut' by default.\n",
    "    sig = inspect.signature(calculate_percentile_cut)\n",
    "    kwargs = dict(values=np.asarray(values, float), bins=np.asarray(bins, float), bin_values=np.asarray(bin_values, float))\n",
    "    target = (1.0 - eff_keep) if keep_upper_tail else eff_keep\n",
    "    try:\n",
    "        if \"efficiency\" in sig.parameters:\n",
    "            return calculate_percentile_cut(**kwargs, efficiency=target)\n",
    "        else:\n",
    "            return calculate_percentile_cut(**kwargs, percentile=target * 100.0)\n",
    "    except Exception as e:\n",
    "        # manual per-bin percentile fallback\n",
    "        edges = np.asarray(bins, float)\n",
    "        cut = np.full(len(edges)-1, np.nan)\n",
    "        for i in range(len(edges)-1):\n",
    "            m = (kwargs[\"bin_values\"] >= edges[i]) & (kwargs[\"bin_values\"] < edges[i+1])\n",
    "            if np.any(m):\n",
    "                pct = (1.0 - eff_keep) * 100.0 if keep_upper_tail else eff_keep * 100.0\n",
    "                cut[i] = np.percentile(kwargs[\"values\"][m], pct)\n",
    "        return cut\n",
    "\n",
    "gh_cut = safe_percentile_cut(gh, e_true, e_true_edges_TeV, eff_keep=gh_eff, keep_upper_tail=True)\n",
    "th_cut = safe_percentile_cut(theta_deg, e_true, e_true_edges_TeV, eff_keep=th_eff, keep_upper_tail=False)\n",
    "\n",
    "# evaluate selection per bin using pyirf (falls back to manual if needed)\n",
    "try:\n",
    "    sel_gh = evaluate_binned_cut(values=gh,    bins=e_true_edges_TeV, bin_values=e_true, cut=gh_cut, operator=\">=\")\n",
    "    sel_th = evaluate_binned_cut(values=theta_deg, bins=e_true_edges_TeV, bin_values=e_true, cut=th_cut, operator=\"<=\")\n",
    "except Exception:\n",
    "    idx = np.digitize(e_true, e_true_edges_TeV) - 1\n",
    "    sel_gh = np.zeros_like(gh, dtype=bool); sel_th = np.zeros_like(gh, dtype=bool)\n",
    "    good = (idx >= 0) & (idx < len(gh_cut))\n",
    "    sel_gh[good] = gh[good] >= gh_cut[idx[good]]\n",
    "    sel_th[good] = theta_deg[good] <= th_cut[idx[good]]\n",
    "\n",
    "sel = sel_gh & sel_th\n",
    "\n",
    "# print compact summary\n",
    "med_gh_cut = float(np.nanmedian(gh_cut))\n",
    "med_th_cut = float(np.nanmedian(th_cut))\n",
    "kept = int(np.count_nonzero(sel))\n",
    "print(f\"Per-bin cuts computed: gh_eff={gh_eff:.2f} (upper-tail) → median cut={med_gh_cut:.3f}; \"\n",
    "      f\"th_eff={th_eff:.2f} → median cut={med_th_cut:.3f} deg\")\n",
    "print(f\"Selected events: {kept} / {len(gh)}  ({kept/len(gh)*100:.1f}%)\")\n",
    "\n",
    "# show a few bin-wise stats\n",
    "edges = e_true_edges_TeV\n",
    "bin_ids = np.digitize(e_true, edges) - 1\n",
    "counts = np.array([np.count_nonzero(bin_ids==i) for i in range(len(edges)-1)])\n",
    "kept_counts = np.array([np.count_nonzero(sel & (bin_ids==i)) for i in range(len(edges)-1)])\n",
    "nonempty = counts > 0\n",
    "print(\"Bins with data:\", int(np.count_nonzero(nonempty)), \"/\", len(counts))\n",
    "print(\"First 8 bins (Etrue_min→max TeV):\")\n",
    "for i in range(min(8, len(counts))):\n",
    "    print(f\"  [{edges[i]:.3f}, {edges[i+1]:.3f})  N={counts[i]:6d}  kept={kept_counts[i]:6d}  gh_cut={gh_cut[i]:.3f}  th_cut={th_cut[i]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b49dbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aeff(after cuts) shape: (16,)\n",
      "Nonzero bins: 16 / 16\n",
      "First 10 Aeff(m^2) after cuts: [  552315.87  1168804.94  1915582.05  2706085.83  3612294.26  4784375.59\n",
      "  6068947.27  7574013.15  9568017.7  11877621.61]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from astropy.table import Table\n",
    "from pyirf.irf import effective_area_per_energy\n",
    "from pyirf.simulations import SimulatedEventsInfo\n",
    "import inspect\n",
    "\n",
    "# Rebuild SimulatedEventsInfo (handles both pyirf signatures)\n",
    "sig = inspect.signature(SimulatedEventsInfo)\n",
    "params = set(sig.parameters.keys())\n",
    "\n",
    "kwargs = dict(\n",
    "    n_showers=349_860_000,       # from metadata\n",
    "    energy_min=0.001 * u.TeV,\n",
    "    energy_max=1000.0 * u.TeV,\n",
    "    max_impact=1760.0 * u.m,\n",
    "    spectral_index=-2.0,\n",
    ")\n",
    "if {\"viewcone_min\", \"viewcone_max\"} <= params:\n",
    "    kwargs.update(viewcone_min=0.0 * u.deg, viewcone_max=0.0 * u.deg)\n",
    "elif \"viewcone\" in params:\n",
    "    kwargs.update(viewcone=0.0 * u.deg)\n",
    "\n",
    "sim_info = SimulatedEventsInfo(**kwargs)\n",
    "\n",
    "# Build selected events table (pyirf needs Quantity 'true_energy')\n",
    "def to_value_tev(col):\n",
    "    try:\n",
    "        return np.asarray(col.to_value(u.TeV), dtype=float)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return np.asarray(col.quantity.to_value(u.TeV), dtype=float)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return np.asarray(col, dtype=float)\n",
    "\n",
    "true_e = to_value_tev(tab[\"true_energy\"])\n",
    "\n",
    "# 'sel' and 'true_bins' are from the previous cell\n",
    "t_selected = Table()\n",
    "t_selected[\"true_energy\"] = (true_e[sel] * u.TeV)\n",
    "\n",
    "aeff_sel = effective_area_per_energy(\n",
    "    selected_events=t_selected,\n",
    "    simulation_info=sim_info,\n",
    "    true_energy_bins=true_bins\n",
    ").to_value(u.m**2)\n",
    "\n",
    "print(\"Aeff(after cuts) shape:\", aeff_sel.shape)\n",
    "print(\"Nonzero bins:\", int(np.count_nonzero(aeff_sel)), \"/\", len(aeff_sel))\n",
    "print(\"First 10 Aeff(m^2) after cuts:\", np.round(aeff_sel[:10], 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7196741b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin  Etrue_min–Etrue_max [TeV]  Nsel    mu_loc    mu_scale     mu_a    model\n",
      "00   0.989–1.245     4131     0.0323      0.1006     1.986   log10_skewnorm\n",
      "01   1.245–1.567     6944     0.0017      0.0903     1.540   log10_skewnorm\n",
      "02   1.567–1.973     9040    -0.0073      0.0407       nan   log10_moyal\n",
      "03   1.973–2.483    10144    -0.0238      0.0407       nan   log10_moyal\n",
      "04   2.483–3.126    10756    -0.0408      0.0822     1.333   log10_skewnorm\n",
      "05   3.126–3.936    11316    -0.0351      0.0407       nan   log10_moyal\n",
      "06   3.936–4.955    11402    -0.0366      0.0403       nan   log10_moyal\n",
      "07   4.955–6.238    11303    -0.0392      0.0402       nan   log10_moyal\n",
      "08   6.238–7.853    11342    -0.0394      0.0402       nan   log10_moyal\n",
      "09   7.853–9.886    11184    -0.0410      0.0395       nan   log10_moyal\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import moyal, skewnorm\n",
    "import astropy.units as u\n",
    "\n",
    "# uses: tab, sel, true_bins from earlier cells\n",
    "\n",
    "def to_value_tev(col):\n",
    "    try:\n",
    "        return np.asarray(col.to_value(u.TeV), dtype=float)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return np.asarray(col.quantity.to_value(u.TeV), dtype=float)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return np.asarray(col, dtype=float)\n",
    "\n",
    "true_e_all = to_value_tev(tab[\"true_energy\"])\n",
    "reco_e_all = to_value_tev(tab[\"reco_energy\"])\n",
    "\n",
    "true_edges = true_bins.to_value(u.TeV)\n",
    "\n",
    "mu_loc = np.full(len(true_edges)-1, np.nan)\n",
    "mu_scale = np.full(len(true_edges)-1, np.nan)\n",
    "mu_a = np.full(len(true_edges)-1, np.nan)\n",
    "model = np.array([\"log10_moyal\"]*(len(true_edges)-1), dtype=object)\n",
    "n_used = np.zeros(len(true_edges)-1, dtype=int)\n",
    "\n",
    "for i in range(len(true_edges)-1):\n",
    "    m = sel & (true_e_all >= true_edges[i]) & (true_e_all < true_edges[i+1])\n",
    "    n_used[i] = int(np.count_nonzero(m))\n",
    "    if n_used[i] < 20:\n",
    "        continue\n",
    "    rel = np.log10(np.clip(reco_e_all[m], 1e-20, None) / np.clip(true_e_all[m], 1e-20, None))\n",
    "    med = np.nanmedian(rel)\n",
    "    mad = np.nanmedian(np.abs(rel - med))\n",
    "    sigma = max(1.4826 * mad, 1e-3)\n",
    "    clean = rel[np.abs(rel - med) < 3.0 * sigma]\n",
    "    if clean.size < 20:\n",
    "        clean = rel\n",
    "\n",
    "    # try moyal\n",
    "    try:\n",
    "        loc_m, scale_m = moyal.fit(clean, loc=med, scale=sigma)\n",
    "    except Exception:\n",
    "        loc_m, scale_m = med, sigma\n",
    "    ok_m = np.isfinite(loc_m) and np.isfinite(scale_m) and (abs(loc_m) < 2.0) and (0.005 < scale_m < 3.0)\n",
    "\n",
    "    # try skewnorm\n",
    "    try:\n",
    "        a_s, loc_s, scale_s = skewnorm.fit(clean, loc=med, scale=sigma)\n",
    "        ok_s = (np.isfinite(a_s) and np.isfinite(loc_s) and np.isfinite(scale_s)\n",
    "                and abs(a_s) < 30 and abs(loc_s) < 2.0 and 0.005 < scale_s < 3.0)\n",
    "    except Exception:\n",
    "        ok_s = False\n",
    "        a_s = loc_s = scale_s = np.nan\n",
    "\n",
    "    if ok_s and (abs(a_s) > 0.5):\n",
    "        mu_loc[i], mu_scale[i], mu_a[i] = float(loc_s), float(scale_s), float(a_s)\n",
    "        model[i] = \"log10_skewnorm\"\n",
    "    elif ok_m:\n",
    "        mu_loc[i], mu_scale[i] = float(loc_m), float(scale_m)\n",
    "        mu_a[i] = np.nan\n",
    "        model[i] = \"log10_moyal\"\n",
    "    else:\n",
    "        mu_loc[i], mu_scale[i] = float(med), float(sigma)\n",
    "        mu_a[i] = np.nan\n",
    "        model[i] = \"log10_moyal\"\n",
    "\n",
    "# print compact table for first 10 bins\n",
    "print(\"Bin  Etrue_min–Etrue_max [TeV]  Nsel    mu_loc    mu_scale     mu_a    model\")\n",
    "for i in range(min(10, len(mu_loc))):\n",
    "    print(f\"{i:02d}   {true_edges[i]:.3f}–{true_edges[i+1]:.3f}    {n_used[i]:5d}   \"\n",
    "          f\"{mu_loc[i]:8.4f}   {mu_scale[i]:9.4f}   {mu_a[i]:7.3f}   {model[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "361379d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma file: gamma_200_300E3GeV_20_20deg_testing_dl1_dl2.h5\n",
      "Zenith token: _20_20deg_\n",
      "Found 1 proton files\n",
      "First few: ['proton_400_500E3GeV_20_20deg_testing_dl1_dl2.h5']\n",
      "Inspecting proton file: proton_400_500E3GeV_20_20deg_testing_dl1_dl2.h5\n",
      "Proton parameter tables: ['/dl2/event/telescope/parameters/stereo']\n",
      "Proton rows: 2754879 | cols: 35\n",
      "Missing expected columns: []\n",
      "Proton reco_energy range [TeV]: 0.5921603736083508 → 284.53092990263553\n"
     ]
    }
   ],
   "source": [
    "import re, glob, h5py\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from ctapipe.io import read_table\n",
    "\n",
    "# 1) derive the zenith token from the gamma file name (e.g. \"_20_20deg_\")\n",
    "name = Path(first_file).name\n",
    "m = re.search(r\"_[0-9]{1,2}_[0-9]{1,2}deg_\", name)\n",
    "zen_token = m.group(0) if m else \"\"\n",
    "print(\"Gamma file:\", name)\n",
    "print(\"Zenith token:\", zen_token or \"(none)\")\n",
    "\n",
    "# 2) find proton files in DL2_DIR matching same zenith token (fallback: any proton_*.h5)\n",
    "proton_files = sorted(glob.glob(str(Path(DL2_DIR) / f\"proton*{zen_token}*.h5\"))) if zen_token else []\n",
    "if not proton_files:\n",
    "    proton_files = sorted(glob.glob(str(Path(DL2_DIR) / \"proton_*.h5\")))\n",
    "print(f\"Found {len(proton_files)} proton files\")\n",
    "print(\"First few:\", [Path(f).name for f in proton_files[:5]])\n",
    "\n",
    "assert proton_files, \"No proton files found.\"\n",
    "\n",
    "pfile = proton_files[0]\n",
    "print(\"Inspecting proton file:\", Path(pfile).name)\n",
    "\n",
    "# 3) find available parameter tables under /dl2/event/telescope/parameters\n",
    "def list_parameter_tables(h5file):\n",
    "    out = []\n",
    "    with h5py.File(h5file, \"r\") as h5:\n",
    "        base = \"/dl2/event/telescope/parameters\"\n",
    "        if base in h5:\n",
    "            for name in h5[base].keys():\n",
    "                out.append(f\"{base}/{name}\")\n",
    "    return out\n",
    "\n",
    "pcands = list_parameter_tables(pfile)\n",
    "print(\"Proton parameter tables:\", pcands)\n",
    "\n",
    "# prefer stereo if present\n",
    "ptable = next((p for p in pcands if p.endswith(\"/stereo\")), (pcands[0] if pcands else None))\n",
    "assert ptable, \"No proton parameter table found.\"\n",
    "\n",
    "# 4) load table and inspect\n",
    "ptab = read_table(pfile, ptable)\n",
    "print(f\"Proton rows: {len(ptab)} | cols: {len(ptab.colnames)}\")\n",
    "needed = [\"reco_energy\",\"gammaness\",\"true_alt\",\"true_az\",\"reco_alt\",\"reco_az\"]\n",
    "missing = [c for c in needed if c not in ptab.colnames]\n",
    "print(\"Missing expected columns:\", missing)\n",
    "\n",
    "def to_value_tev(col):\n",
    "    try:\n",
    "        return np.asarray(col.to_value(u.TeV), dtype=float)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return np.asarray(col.quantity.to_value(u.TeV), dtype=float)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return np.asarray(col, dtype=float)\n",
    "\n",
    "ereco = to_value_tev(ptab[\"reco_energy\"]) if \"reco_energy\" in ptab.colnames else None\n",
    "if ereco is not None:\n",
    "    print(\"Proton reco_energy range [TeV]:\", float(np.nanmin(ereco)), \"→\", float(np.nanmax(ereco)))\n",
    "else:\n",
    "    print(\"No proton 'reco_energy' to summarize.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea8e0ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proton selected: 60298 / 2754879 (2.2%)\n",
      "Reco-energy bins: 13   Nonzero rate bins: 13\n",
      "First 10 bins summary:\n",
      "  [1.201, 1.602)  N_sel=   2708  rate=0.904491 Hz\n",
      "  [1.602, 2.136)  N_sel=   4763  rate=1.590875 Hz\n",
      "  [2.136, 2.848)  N_sel=   4955  rate=1.655004 Hz\n",
      "  [2.848, 3.798)  N_sel=   4874  rate=1.627950 Hz\n",
      "  [3.798, 5.065)  N_sel=   5161  rate=1.723810 Hz\n",
      "  [5.065, 6.754)  N_sel=   5728  rate=1.913192 Hz\n",
      "  [6.754, 9.007)  N_sel=   6485  rate=2.166035 Hz\n",
      "  [9.007, 12.011)  N_sel=   6996  rate=2.336712 Hz\n",
      "  [12.011, 16.016)  N_sel=   7271  rate=2.428564 Hz\n",
      "  [16.016, 21.358)  N_sel=   6443  rate=2.152007 Hz\n",
      "Display cuts → Gammaness_cut=0.698, Theta_cut_deg=13.270\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from pyirf.cuts import evaluate_binned_cut\n",
    "from pyirf.binning import create_bins_per_decade\n",
    "\n",
    "# Inputs reused from previous cells:\n",
    "# - tab : gamma table (already loaded)\n",
    "# - ptab: proton table (already loaded)\n",
    "# - true_bins, e_true_edges_TeV : true-energy bin edges (from earlier)\n",
    "# - gh_cut, th_cut : per-true-E-bin cuts we computed on gamma\n",
    "# - theta_from_altaz, to_value_rad, to_value_tev : helpers defined earlier (redefine briefly here)\n",
    "\n",
    "def to_value_rad(col):\n",
    "    try:\n",
    "        return np.asarray(col.to_value(u.rad))\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return np.asarray(col.quantity.to_value(u.rad))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return np.asarray(col)\n",
    "\n",
    "def to_value_tev(col):\n",
    "    try:\n",
    "        return np.asarray(col.to_value(u.TeV), dtype=float)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return np.asarray(col.quantity.to_value(u.TeV), dtype=float)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return np.asarray(col, dtype=float)\n",
    "\n",
    "def theta_from_altaz(true_alt, true_az, reco_alt, reco_az):\n",
    "    talt = to_value_rad(true_alt); taz = to_value_rad(true_az)\n",
    "    ralt = to_value_rad(reco_alt); raz = to_value_rad(reco_az)\n",
    "    cos_th = (np.sin(talt)*np.sin(ralt) + np.cos(talt)*np.cos(ralt)*np.cos(taz-raz))\n",
    "    cos_th = np.clip(cos_th, -1.0, 1.0)\n",
    "    return np.arccos(cos_th)\n",
    "\n",
    "# 1) Selection masks on protons using the gamma-derived cuts (per TRUE-E bin)\n",
    "p_true = to_value_tev(ptab[\"true_energy\"])\n",
    "p_gh   = np.asarray(ptab[\"gammaness\"], dtype=float)\n",
    "p_th   = np.rad2deg(theta_from_altaz(ptab[\"true_alt\"], ptab[\"true_az\"], ptab[\"reco_alt\"], ptab[\"reco_az\"]))\n",
    "\n",
    "try:\n",
    "    p_sel_gh = evaluate_binned_cut(values=p_gh, bins=e_true_edges_TeV, bin_values=p_true, cut=gh_cut, operator=\">=\")\n",
    "    p_sel_th = evaluate_binned_cut(values=p_th, bins=e_true_edges_TeV, bin_values=p_true, cut=th_cut, operator=\"<=\")\n",
    "except Exception:\n",
    "    idx = np.digitize(p_true, e_true_edges_TeV) - 1\n",
    "    p_sel_gh = np.zeros_like(p_gh, dtype=bool); p_sel_th = np.zeros_like(p_gh, dtype=bool)\n",
    "    good = (idx >= 0) & (idx < len(gh_cut))\n",
    "    p_sel_gh[good] = p_gh[good] >= gh_cut[idx[good]]\n",
    "    p_sel_th[good] = p_th[good] <= th_cut[idx[good]]\n",
    "\n",
    "p_sel = p_sel_gh & p_sel_th\n",
    "print(f\"Proton selected: {int(np.count_nonzero(p_sel))} / {len(p_sel)} \"\n",
    "      f\"({100*np.count_nonzero(p_sel)/len(p_sel):.1f}%)\")\n",
    "\n",
    "# 2) Background shape over RECO-energy bins (distribute total rate)\n",
    "p_reco = to_value_tev(ptab[\"reco_energy\"])\n",
    "# sensible reco binning from data range\n",
    "pr_emin = max(0.05, float(np.nanpercentile(p_reco, 0.5)))\n",
    "pr_emax = min(60.0, float(np.nanpercentile(p_reco, 99.5)))\n",
    "e_reco_bins = create_bins_per_decade((pr_emin*u.TeV), (pr_emax*u.TeV), bins_per_decade=8).to_value(u.TeV)\n",
    "\n",
    "counts_sel, _ = np.histogram(p_reco[p_sel], bins=e_reco_bins)\n",
    "total_rate_hz = 20.0  # choose a realistic ROI total rate for a quick sanity check\n",
    "rates = total_rate_hz * counts_sel.astype(float) / max(1, counts_sel.sum())\n",
    "rates = np.maximum(rates, 1e-12)\n",
    "\n",
    "print(\"Reco-energy bins:\", len(e_reco_bins)-1, \"  Nonzero rate bins:\", int(np.count_nonzero(rates)))\n",
    "print(\"First 10 bins summary:\")\n",
    "for i in range(min(10, len(rates))):\n",
    "    print(f\"  [{e_reco_bins[i]:.3f}, {e_reco_bins[i+1]:.3f})  N_sel={counts_sel[i]:7d}  rate={rates[i]:.6f} Hz\")\n",
    "\n",
    "# Also report display cuts (medians) that we would store in the background CSV\n",
    "gh_display = float(np.nanmedian(gh_cut))\n",
    "th_display = float(np.nanmedian(th_cut))\n",
    "print(f\"Display cuts → Gammaness_cut={gh_display:.3f}, Theta_cut_deg={th_display:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "24728184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zenith angle (deg): 20\n",
      "Saved gamma CSV: SST1M_csv\\SST1M_gamma_irf_gheffi_0.70_theffi_0.70.csv\n",
      "Saved backg CSV: SST1M_csv\\SST1M_backg_irf_gheffi_0.70_theffi_0.70.csv\n",
      "Gamma head:\n",
      "    ZD_deg  Etrue_min_TeV  Etrue_max_TeV       Aeff_m2  emig_mu_loc  \\\n",
      "0    20.0       0.988613       1.244591  5.523159e+05     0.032257   \n",
      "1    20.0       1.244591       1.566847  1.168805e+06     0.001696   \n",
      "2    20.0       1.566847       1.972543  1.915582e+06    -0.007345   \n",
      "\n",
      "   emig_mu_scale  emig_mu_a      emig_model  \n",
      "0       0.100580    1.98594  log10_skewnorm  \n",
      "1       0.090290    1.54035  log10_skewnorm  \n",
      "2       0.040725        NaN     log10_moyal  \n",
      "Backg head:\n",
      "    ZD_deg  Ereco_min_TeV  Ereco_max_TeV  BckgRate_per_second  Theta_cut_deg  \\\n",
      "0    20.0       1.201065       1.601645             0.904491       13.27004   \n",
      "1    20.0       1.601645       2.135828             1.590875       13.27004   \n",
      "2    20.0       2.135828       2.848173             1.655004       13.27004   \n",
      "\n",
      "   Gammaness_cut  \n",
      "0       0.697983  \n",
      "1       0.697983  \n",
      "2       0.697983  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import astropy.units as u\n",
    "\n",
    "# --- inputs reused from previous cells ---\n",
    "# first_file (gamma), pfile (proton), tab (gamma table), ptab (proton table)\n",
    "# true_bins, aeff_sel\n",
    "# mu_loc, mu_scale, mu_a, model (from migration fit)\n",
    "# e_reco_bins, rates, gh_display, th_display (from background shape step)\n",
    "\n",
    "# 1) derive zenith from filename\n",
    "name = Path(first_file).name\n",
    "m = re.search(r\"_([0-9]{1,2})_([0-9]{1,2})deg_\", name)\n",
    "zenith = int(m.group(1)) if m else -1\n",
    "print(\"Zenith angle (deg):\", zenith)\n",
    "\n",
    "# 2) edges for gamma CSV\n",
    "e_true_edges_TeV = true_bins.to_value(u.TeV)\n",
    "\n",
    "# 3) assemble gamma CSV rows\n",
    "gamma_rows = []\n",
    "for i in range(len(e_true_edges_TeV) - 1):\n",
    "    row = {\n",
    "        \"ZD_deg\": float(zenith),\n",
    "        \"Etrue_min_TeV\": float(e_true_edges_TeV[i]),\n",
    "        \"Etrue_max_TeV\": float(e_true_edges_TeV[i+1]),\n",
    "        \"Aeff_m2\": float(max(aeff_sel[i], 0.0)),\n",
    "        \"emig_mu_loc\": float(mu_loc[i]) if np.isfinite(mu_loc[i]) else 0.0,\n",
    "        \"emig_mu_scale\": float(mu_scale[i]) if np.isfinite(mu_scale[i]) else 0.2,\n",
    "        \"emig_mu_a\": float(mu_a[i]) if np.isfinite(mu_a[i]) else np.nan,\n",
    "        \"emig_model\": str(model[i]),\n",
    "    }\n",
    "    gamma_rows.append(row)\n",
    "df_gamma = pd.DataFrame(gamma_rows)\n",
    "\n",
    "# 4) assemble background CSV rows\n",
    "back_rows = []\n",
    "for i in range(len(e_reco_bins) - 1):\n",
    "    row = {\n",
    "        \"ZD_deg\": float(zenith),\n",
    "        \"Ereco_min_TeV\": float(e_reco_bins[i]),\n",
    "        \"Ereco_max_TeV\": float(e_reco_bins[i+1]),\n",
    "        \"BckgRate_per_second\": float(rates[i]),\n",
    "        \"Theta_cut_deg\": float(th_display),\n",
    "        \"Gammaness_cut\": float(gh_display),\n",
    "    }\n",
    "    back_rows.append(row)\n",
    "df_back = pd.DataFrame(back_rows)\n",
    "\n",
    "# 5) save to disk (adjust OUTPUT_DIR if you like)\n",
    "OUTPUT_DIR = Path(r\".\\SST1M_csv\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "gh_eff = 0.70\n",
    "th_eff = 0.70\n",
    "gamma_csv = OUTPUT_DIR / f\"SST1M_gamma_irf_gheffi_{gh_eff:.2f}_theffi_{th_eff:.2f}.csv\"\n",
    "backg_csv = OUTPUT_DIR / f\"SST1M_backg_irf_gheffi_{gh_eff:.2f}_theffi_{th_eff:.2f}.csv\"\n",
    "\n",
    "# append if files exist, otherwise write with header\n",
    "if gamma_csv.exists():\n",
    "    df_gamma.to_csv(gamma_csv, mode=\"a\", header=False, index=False)\n",
    "else:\n",
    "    df_gamma.to_csv(gamma_csv, index=False)\n",
    "\n",
    "if backg_csv.exists():\n",
    "    df_back.to_csv(backg_csv, mode=\"a\", header=False, index=False)\n",
    "else:\n",
    "    df_back.to_csv(backg_csv, index=False)\n",
    "\n",
    "print(\"Saved gamma CSV:\", gamma_csv)\n",
    "print(\"Saved backg CSV:\", backg_csv)\n",
    "print(\"Gamma head:\\n\", df_gamma.head(3))\n",
    "print(\"Backg head:\\n\", df_back.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ece2dc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[zd=20 eff=0.40] rows: back=14, gamma=16\n",
      "[zd=30 eff=0.40] rows: back=13, gamma=15\n",
      "[zd=40 eff=0.40] rows: back=12, gamma=13\n",
      "[zd=60 eff=0.40] rows: back=7, gamma=7\n",
      "[zd=20 eff=0.70] rows: back=14, gamma=16\n",
      "[zd=30 eff=0.70] rows: back=13, gamma=15\n",
      "[zd=40 eff=0.70] rows: back=12, gamma=13\n",
      "[zd=60 eff=0.70] rows: back=7, gamma=7\n",
      "[zd=20 eff=0.90] rows: back=14, gamma=16\n",
      "[zd=30 eff=0.90] rows: back=13, gamma=15\n",
      "[zd=40 eff=0.90] rows: back=12, gamma=13\n",
      "[zd=60 eff=0.90] rows: back=7, gamma=7\n",
      "Done. CSVs in: C:\\Users\\aminnakh\\Desktop\\nakhle backup2\\a.nakhle\\sst1mpipe\\sst1mpipe\\source_simulation\\scripts\\final_CSV\n"
     ]
    }
   ],
   "source": [
    "# ONE-CELL PIPELINE: build IRF CSVs for all zeniths using gamma + gamma_point (signal) and proton (background)\n",
    "# Efficiencies: 0.40, 0.70, 0.90\n",
    "# Output: ./final_CSV/SST1M_gamma_irf_gheffi_XX_theffi_XX.csv and SST1M_backg_irf_gheffi_XX_theffi_XX.csv\n",
    "\n",
    "import re, glob, inspect\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import astropy.units as u\n",
    "from astropy.table import vstack, Table\n",
    "import h5py\n",
    "\n",
    "from ctapipe.io import read_table\n",
    "\n",
    "from pyirf.cuts import calculate_percentile_cut, evaluate_binned_cut\n",
    "from pyirf.binning import create_bins_per_decade\n",
    "from pyirf.irf import effective_area_per_energy\n",
    "from pyirf.simulations import SimulatedEventsInfo\n",
    "\n",
    "# ---------------- utilities ----------------\n",
    "def zd_from_filename(path):\n",
    "    m = re.search(r\"_([0-9]{1,2})_([0-9]{1,2})deg_\", Path(path).name)\n",
    "    if not m:\n",
    "        return None\n",
    "    return float(int(m.group(1)))\n",
    "\n",
    "def to_value_tev(col):\n",
    "    for attr in (\"to_value\", \"quantity\"):\n",
    "        try:\n",
    "            if attr == \"to_value\": return np.asarray(col.to_value(u.TeV), dtype=float)\n",
    "            return np.asarray(col.quantity.to_value(u.TeV), dtype=float)\n",
    "        except Exception: pass\n",
    "    return np.asarray(col, dtype=float)\n",
    "\n",
    "def to_value_rad(col):\n",
    "    for attr in (\"to_value\", \"quantity\"):\n",
    "        try:\n",
    "            if attr == \"to_value\": return np.asarray(col.to_value(u.rad))\n",
    "            return np.asarray(col.quantity.to_value(u.rad))\n",
    "        except Exception: pass\n",
    "    return np.asarray(col)\n",
    "\n",
    "def theta_from_altaz(true_alt, true_az, reco_alt, reco_az):\n",
    "    talt = to_value_rad(true_alt); taz = to_value_rad(true_az)\n",
    "    ralt = to_value_rad(reco_alt); raz = to_value_rad(reco_az)\n",
    "    cos_th = (np.sin(talt)*np.sin(ralt) + np.cos(talt)*np.cos(ralt)*np.cos(taz-raz))\n",
    "    return np.arccos(np.clip(cos_th, -1.0, 1.0))\n",
    "\n",
    "def read_param_table(path):\n",
    "    with h5py.File(path, \"r\") as h5:\n",
    "        base = \"/dl2/event/telescope/parameters\"\n",
    "        if base not in h5:\n",
    "            raise KeyError(f\"{base} missing in {path}\")\n",
    "        # prefer stereo\n",
    "        cand = [f\"{base}/stereo\"] + [f\"{base}/{k}\" for k in h5[base].keys() if k != \"stereo\"]\n",
    "        for p in cand:\n",
    "            if p in h5:\n",
    "                return read_table(path, p)\n",
    "    raise KeyError(\"No parameters table found\")\n",
    "\n",
    "def stack_tables(paths):\n",
    "    tabs = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            t = read_param_table(p)\n",
    "            t.meta = {}\n",
    "            tabs.append(t)\n",
    "        except Exception as e:\n",
    "            print(f\"[skip] {Path(p).name}: {e}\")\n",
    "    if not tabs:\n",
    "        return None\n",
    "    return vstack(tabs, metadata_conflicts=\"silent\")\n",
    "\n",
    "def read_shower_info(path):\n",
    "    try:\n",
    "        sd = read_table(path, \"/simulation/service/shower_distribution\")\n",
    "    except Exception:\n",
    "        return None\n",
    "    out = {}\n",
    "    out[\"n_entries\"] = int(np.nansum(np.asarray(sd[\"n_entries\"]))) if \"n_entries\" in sd.colnames else 0\n",
    "    if \"bins_energy\" in sd.colnames:\n",
    "        eb = sd[\"bins_energy\"][0]\n",
    "        out[\"emin\"] = float(np.min(eb)); out[\"emax\"] = float(np.max(eb))\n",
    "    if \"bins_core_dist\" in sd.colnames:\n",
    "        cb = sd[\"bins_core_dist\"][0]\n",
    "        out[\"rmax\"] = float(np.max(cb))\n",
    "    return out\n",
    "\n",
    "def aggregate_sim_info(paths):\n",
    "    tot_n = 0; emin = None; emax = None; rmax = None\n",
    "    for p in paths:\n",
    "        info = read_shower_info(p)\n",
    "        if not info: continue\n",
    "        tot_n += info.get(\"n_entries\", 0)\n",
    "        if \"emin\" in info:\n",
    "            emin = info[\"emin\"] if emin is None else min(emin, info[\"emin\"])\n",
    "        if \"emax\" in info:\n",
    "            emax = info[\"emax\"] if emax is None else max(emax, info[\"emax\"])\n",
    "        if \"rmax\" in info:\n",
    "            rmax = info[\"rmax\"] if rmax is None else max(rmax, info[\"rmax\"])\n",
    "    # conservative fallbacks\n",
    "    if emin is None: emin = 0.001\n",
    "    if emax is None: emax = 1000.0\n",
    "    if rmax is None: rmax = 300.0\n",
    "    return dict(n_showers=tot_n, emin=emin, emax=emax, rmax=rmax)\n",
    "\n",
    "def sim_info_from_agg(agg):\n",
    "    sig = inspect.signature(SimulatedEventsInfo)\n",
    "    params = set(sig.parameters.keys())\n",
    "    kwargs = dict(\n",
    "        n_showers=int(max(agg[\"n_showers\"], 1)),\n",
    "        energy_min=float(agg[\"emin\"]) * u.TeV,\n",
    "        energy_max=float(agg[\"emax\"]) * u.TeV,\n",
    "        max_impact=float(agg[\"rmax\"]) * u.m,\n",
    "        spectral_index=-2.0,\n",
    "    )\n",
    "    if {\"viewcone_min\",\"viewcone_max\"} <= params:\n",
    "        kwargs.update(viewcone_min=0.0*u.deg, viewcone_max=0.0*u.deg)\n",
    "    elif \"viewcone\" in params:\n",
    "        kwargs.update(viewcone=0.0*u.deg)\n",
    "    return SimulatedEventsInfo(**kwargs)\n",
    "\n",
    "def as_float_array(x):\n",
    "    if hasattr(x, 'colnames'):\n",
    "        name = 'cut' if 'cut' in x.colnames else x.colnames[0]\n",
    "        col = x[name]\n",
    "        try: return np.asarray(col.to_value(u.one), dtype=float)\n",
    "        except Exception: return np.asarray(col, dtype=float)\n",
    "    if isinstance(x, np.ndarray) and x.dtype.names:\n",
    "        fld = 'cut' if 'cut' in x.dtype.names else x.dtype.names[0]\n",
    "        return np.asarray(x[fld], dtype=float)\n",
    "    try: return np.asarray(x.to_value(u.one), dtype=float)\n",
    "    except Exception: return np.asarray(x, dtype=float)\n",
    "\n",
    "def percentile_cut_compat(values, bin_values, edges, eff_keep, upper_tail=False):\n",
    "    sig = inspect.signature(calculate_percentile_cut)\n",
    "    params = set(sig.parameters.keys())\n",
    "    kwargs = dict(values=np.asarray(values, float),\n",
    "                  bins=np.asarray(edges, float),\n",
    "                  bin_values=np.asarray(bin_values, float))\n",
    "    target = (1.0 - eff_keep) if upper_tail else eff_keep\n",
    "    if \"efficiency\" in params:\n",
    "        kwargs[\"efficiency\"] = target\n",
    "    else:\n",
    "        kwargs[\"percentile\"] = target * 100.0\n",
    "    if \"fill_value\" in params:\n",
    "        kwargs[\"fill_value\"] = np.nan\n",
    "    return as_float_array(calculate_percentile_cut(**kwargs))\n",
    "\n",
    "def eval_binned(values, bin_values, edges, cut, op):\n",
    "    cut_arr = as_float_array(cut)\n",
    "    try:\n",
    "        return evaluate_binned_cut(values=np.asarray(values,float),\n",
    "                                   bins=np.asarray(edges,float),\n",
    "                                   bin_values=np.asarray(bin_values,float),\n",
    "                                   cut=cut_arr,\n",
    "                                   operator=op)\n",
    "    except Exception:\n",
    "        idx = np.digitize(bin_values, edges) - 1\n",
    "        sel = np.zeros_like(values, dtype=bool)\n",
    "        good = (idx>=0) & (idx<len(cut_arr))\n",
    "        if op == \">=\": sel[good] = values[good] >= cut_arr[idx[good]]\n",
    "        else:          sel[good] = values[good] <= cut_arr[idx[good]]\n",
    "        return sel\n",
    "\n",
    "# ---------------- gather files by zenith ----------------\n",
    "DL2 = Path(DL2_DIR)\n",
    "gamma_files       = sorted(glob.glob(str(DL2 / \"gamma_*.h5\")))\n",
    "gamma_point_files = sorted(glob.glob(str(DL2 / \"gamma_point_*.h5\")))\n",
    "proton_files      = sorted(glob.glob(str(DL2 / \"proton_*.h5\")))\n",
    "\n",
    "def group_by_zenith(paths):\n",
    "    d = {}\n",
    "    for p in paths:\n",
    "        z = zd_from_filename(p)\n",
    "        if z is None: continue\n",
    "        d.setdefault(z, []).append(p)\n",
    "    return d\n",
    "\n",
    "G = group_by_zenith(gamma_files)\n",
    "GP = group_by_zenith(gamma_point_files)\n",
    "P = group_by_zenith(proton_files)\n",
    "\n",
    "zeniths = sorted(set(G.keys()) | set(GP.keys()) | set(P.keys()))\n",
    "if not zeniths:\n",
    "    raise RuntimeError(\"No DL2 files grouped by zenith found.\")\n",
    "\n",
    "# ---------------- build CSVs per efficiency, appending all zeniths ----------------\n",
    "OUT = Path(\"./final_CSV\")\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "eff_list = [0.40, 0.70, 0.90]\n",
    "\n",
    "for eff in eff_list:\n",
    "    gamma_csv = OUT / f\"SST1M_gamma_irf_gheffi_{eff:.2f}_theffi_{eff:.2f}.csv\"\n",
    "    backg_csv = OUT / f\"SST1M_backg_irf_gheffi_{eff:.2f}_theffi_{eff:.2f}.csv\"\n",
    "    if gamma_csv.exists(): gamma_csv.unlink()\n",
    "    if backg_csv.exists(): backg_csv.unlink()\n",
    "\n",
    "    for zd in zeniths:\n",
    "        g_paths = G.get(zd, []) + GP.get(zd, [])\n",
    "        p_paths = P.get(zd, [])\n",
    "        if not g_paths or not p_paths:\n",
    "            print(f\"[skip zd={zd}] gamma_paths={len(g_paths)} proton_paths={len(p_paths)}\")\n",
    "            continue\n",
    "\n",
    "        g_tab = stack_tables(g_paths)\n",
    "        p_tab = stack_tables(p_paths)\n",
    "        if g_tab is None or p_tab is None:\n",
    "            print(f\"[skip zd={zd}] could not stack tables\")\n",
    "            continue\n",
    "\n",
    "        g_true = to_value_tev(g_tab[\"true_energy\"]); g_reco = to_value_tev(g_tab[\"reco_energy\"])\n",
    "        g_gh = np.asarray(g_tab[\"gammaness\"], float)\n",
    "        g_th = np.rad2deg(theta_from_altaz(g_tab[\"true_alt\"], g_tab[\"true_az\"], g_tab[\"reco_alt\"], g_tab[\"reco_az\"]))\n",
    "\n",
    "        p_true = to_value_tev(p_tab[\"true_energy\"]); p_reco = to_value_tev(p_tab[\"reco_energy\"])\n",
    "        p_gh = np.asarray(p_tab[\"gammaness\"], float)\n",
    "        p_th = np.rad2deg(theta_from_altaz(p_tab[\"true_alt\"], p_tab[\"true_az\"], p_tab[\"reco_alt\"], p_tab[\"reco_az\"]))\n",
    "\n",
    "        sim_agg = aggregate_sim_info(g_paths)\n",
    "        sim_g = sim_info_from_agg(sim_agg)\n",
    "\n",
    "        t_emin = max(0.05, float(np.nanpercentile(g_true, 1)))\n",
    "        t_emax = min(40.0, float(np.nanpercentile(g_true, 99.5)))\n",
    "        true_bins = create_bins_per_decade((t_emin*u.TeV), (t_emax*u.TeV), bins_per_decade=10)\n",
    "        e_true_edges = true_bins.to_value(u.TeV)\n",
    "\n",
    "        r_emin = max(0.05, float(np.nanpercentile(p_reco, 0.5)))\n",
    "        r_emax = min(80.0, float(np.nanpercentile(p_reco, 99.7)))\n",
    "        reco_bins = create_bins_per_decade((r_emin*u.TeV), (r_emax*u.TeV), bins_per_decade=8)\n",
    "        e_reco_edges = reco_bins.to_value(u.TeV)\n",
    "\n",
    "        gh_cut_reco = percentile_cut_compat(g_gh, g_reco, e_reco_edges, eff_keep=eff, upper_tail=True)\n",
    "        th_cut_reco = percentile_cut_compat(g_th, g_reco, e_reco_edges, eff_keep=eff, upper_tail=False)\n",
    "\n",
    "        p_sel_gh = eval_binned(p_gh, p_reco, e_reco_edges, gh_cut_reco, op=\">=\")\n",
    "        p_sel_th = eval_binned(p_th, p_reco, e_reco_edges, th_cut_reco, op=\"<=\")\n",
    "        p_sel = p_sel_gh & p_sel_th\n",
    "\n",
    "        counts_sel, _ = np.histogram(p_reco[p_sel], bins=e_reco_edges)\n",
    "        total_rate_hz = 20.0\n",
    "        rates = total_rate_hz * counts_sel.astype(float) / max(1, counts_sel.sum())\n",
    "        rates = np.maximum(rates, 1e-12)\n",
    "\n",
    "        df_back = pd.DataFrame({\n",
    "            \"ZD_deg\": np.full(len(e_reco_edges)-1, zd, dtype=float),\n",
    "            \"Ereco_min_TeV\": e_reco_edges[:-1].astype(float),\n",
    "            \"Ereco_max_TeV\": e_reco_edges[1:].astype(float),\n",
    "            \"BckgRate_per_second\": rates.astype(float),\n",
    "            \"Theta_cut_deg\": th_cut_reco.astype(float),\n",
    "            \"Gammaness_cut\": gh_cut_reco.astype(float),\n",
    "        })\n",
    "        df_back.to_csv(backg_csv, mode=\"a\", header=not backg_csv.exists(), index=False)\n",
    "\n",
    "        gh_cut_true = percentile_cut_compat(g_gh, g_true, e_true_edges, eff_keep=eff, upper_tail=True)\n",
    "        th_cut_true = percentile_cut_compat(g_th, g_true, e_true_edges, eff_keep=eff, upper_tail=False)\n",
    "\n",
    "        g_sel_gh = eval_binned(g_gh, g_true, e_true_edges, gh_cut_true, op=\">=\")\n",
    "        g_sel_th = eval_binned(g_th, g_true, e_true_edges, th_cut_true, op=\"<=\")\n",
    "        g_sel = g_sel_gh & g_sel_th\n",
    "\n",
    "        t_selected = Table()\n",
    "        t_selected[\"true_energy\"] = (g_true[g_sel] * u.TeV)\n",
    "        aeff_sel = effective_area_per_energy(t_selected, sim_g, true_bins).to_value(u.m**2)\n",
    "\n",
    "        # energy-migration fit on ratio Ereco/Etrue per true-E bin\n",
    "        mu_loc = np.full(len(e_true_edges)-1, np.nan)\n",
    "        mu_scale = np.full(len(e_true_edges)-1, np.nan)\n",
    "        mu_a = np.full(len(e_true_edges)-1, np.nan)\n",
    "        model = np.array([\"moyal\"]*(len(e_true_edges)-1), dtype=object)\n",
    "\n",
    "        from scipy.stats import moyal, skewnorm\n",
    "        for i in range(len(e_true_edges)-1):\n",
    "            m = g_sel & (g_true >= e_true_edges[i]) & (g_true < e_true_edges[i+1])\n",
    "            if np.count_nonzero(m) < 20:\n",
    "                continue\n",
    "            ratio = np.clip(g_reco[m] / np.clip(g_true[m], 1e-20, None), 1e-6, 1e6)\n",
    "            med = float(np.nanmedian(ratio))\n",
    "            mad = float(np.nanmedian(np.abs(ratio - med)))\n",
    "            sigma = max(1.4826 * mad, 1e-3)\n",
    "            clean = ratio[np.abs(ratio - med) < 3.0 * sigma]\n",
    "            if clean.size < 20: clean = ratio\n",
    "            try:\n",
    "                loc_m, scale_m = moyal.fit(clean, loc=med, scale=sigma)\n",
    "            except Exception:\n",
    "                loc_m, scale_m = med, sigma\n",
    "            ok_m = np.isfinite(loc_m) and np.isfinite(scale_m) and (0.01 < scale_m < 3.0)\n",
    "            try:\n",
    "                a_s, loc_s, scale_s = skewnorm.fit(clean, loc=med, scale=sigma)\n",
    "                ok_s = (np.isfinite(a_s) and np.isfinite(loc_s) and np.isfinite(scale_s)\n",
    "                        and abs(a_s) < 30 and (0.01 < scale_s < 3.0))\n",
    "            except Exception:\n",
    "                ok_s = False; a_s = loc_s = scale_s = np.nan\n",
    "            if ok_s and (abs(a_s) > 0.5):\n",
    "                mu_loc[i], mu_scale[i], mu_a[i] = float(loc_s), float(scale_s), float(a_s); model[i] = \"skewnorm\"\n",
    "            elif ok_m:\n",
    "                mu_loc[i], mu_scale[i], mu_a[i] = float(loc_m), float(scale_m), np.nan; model[i] = \"moyal\"\n",
    "            else:\n",
    "                mu_loc[i], mu_scale[i], mu_a[i] = med, sigma, np.nan; model[i] = \"moyal\"\n",
    "\n",
    "        df_gamma = pd.DataFrame({\n",
    "            \"ZD_deg\": np.full(len(e_true_edges)-1, zd, dtype=float),\n",
    "            \"Etrue_min_TeV\": e_true_edges[:-1].astype(float),\n",
    "            \"Etrue_max_TeV\": e_true_edges[1:].astype(float),\n",
    "            \"Aeff_m2\": aeff_sel.astype(float),\n",
    "            \"emig_mu_loc\": mu_loc.astype(float),\n",
    "            \"emig_mu_scale\": mu_scale.astype(float),\n",
    "            \"emig_mu_a\": mu_a.astype(float),\n",
    "            \"emig_model\": model,\n",
    "        })\n",
    "        df_gamma.to_csv(gamma_csv, mode=\"a\", header=not gamma_csv.exists(), index=False)\n",
    "\n",
    "        print(f\"[zd={zd:>2.0f} eff={eff:.2f}] rows: back={len(df_back)}, gamma={len(df_gamma)}\")\n",
    "\n",
    "print(\"Done. CSVs in:\", OUT.resolve())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
