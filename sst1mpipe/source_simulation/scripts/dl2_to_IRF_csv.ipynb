{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "897aee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 1\n",
    "\n",
    "DL2_DIR = r\"../dl2_gamma\"\n",
    "PATTERNS = [\"gamma_*.h5\", \"proton_*.h5\"]  \n",
    "\n",
    "import os, glob, h5py, textwrap, inspect\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import astropy.units as u\n",
    "from astropy.table import Table\n",
    "\n",
    "from ctapipe.io import read_table\n",
    "\n",
    "# Optional: matplotlib quick plots\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8982b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 files\n",
      " - gamma_200_300E3GeV_20_20deg_testing_dl1_dl2.h5\n",
      " - gamma_200_300E3GeV_30_30deg_testing_dl1_dl2.h5\n",
      " - gamma_200_300E3GeV_40_40deg_testing_dl1_dl2.h5\n",
      " - gamma_200_300E3GeV_60_60deg_testing_dl1_dl2.h5\n",
      " - gamma_point_50_300E3GeV_20_20deg_testing_dl1_dl2.h5\n",
      " - gamma_point_50_300E3GeV_30_30deg_testing_dl1_dl2.h5\n",
      " - gamma_point_50_300E3GeV_40_40deg_testing_dl1_dl2.h5\n",
      " - gamma_point_50_300E3GeV_60_60deg_testing_dl1_dl2.h5\n",
      " - proton_400_500E3GeV_20_20deg_testing_dl1_dl2.h5\n",
      " - proton_400_500E3GeV_30_30deg_testing_dl1_dl2.h5\n",
      " - proton_400_500E3GeV_40_40deg_testing_dl1_dl2.h5\n",
      " - proton_400_500E3GeV_60_60deg_testing_dl1_dl2.h5\n"
     ]
    }
   ],
   "source": [
    "#cell 2\n",
    "files = []\n",
    "for pat in PATTERNS:\n",
    "    files.extend(sorted(glob.glob(str(Path(DL2_DIR) / pat))))\n",
    "print(f\"Found {len(files)} files\")\n",
    "for f in files:\n",
    "    print(\" -\", Path(f).name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cf4712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting file: gamma_200_300E3GeV_20_20deg_testing_dl1_dl2.h5\n",
      "Found parameter tables:\n",
      " - /dl2/event/telescope/parameters/stereo\n"
     ]
    }
   ],
   "source": [
    "#cell 3\n",
    "# List available DL2 parameter tables in the FIRST matching DL2 file, using your existing DL2_DIR and PATTERNS variables\n",
    "\n",
    "import glob, h5py\n",
    "from pathlib import Path\n",
    "\n",
    "# Use existing variables; fall back gracefully if PATTERNS not defined\n",
    "try:\n",
    "    patterns = PATTERNS\n",
    "except NameError:\n",
    "    patterns = [\"*.h5\"]\n",
    "\n",
    "files = []\n",
    "for pat in patterns:\n",
    "    files.extend(sorted(glob.glob(str(Path(DL2_DIR) / pat))))\n",
    "\n",
    "assert files, f\"No DL2 files found in {DL2_DIR} with patterns {patterns}\"\n",
    "\n",
    "first_file = files[0]\n",
    "print(\"Inspecting file:\", Path(first_file).name)\n",
    "\n",
    "def list_parameter_tables(h5file):\n",
    "    out = []\n",
    "    with h5py.File(h5file, \"r\") as h5:\n",
    "        base = \"/dl2/event/telescope/parameters\"\n",
    "        if base not in h5:\n",
    "            print(\"Not found:\", base)\n",
    "            return out\n",
    "        for name in h5[base].keys():\n",
    "            out.append(f\"{base}/{name}\")\n",
    "    return out\n",
    "\n",
    "candidates = list_parameter_tables(first_file)\n",
    "print(\"Found parameter tables:\")\n",
    "for p in candidates:\n",
    "    print(\" -\", p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "398cd32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 393483 | Columns: 35\n",
      "First 30 columns: ['obs_id', 'event_id', 'true_az_tel', 'true_alt_tel', 'HillasReconstructor_core_x', 'HillasReconstructor_core_y', 'HillasReconstructor_h_max', 'true_az', 'true_alt', 'true_energy', 'log_true_energy', 'true_core_x', 'true_core_y', 'true_h_first_int', 'true_x_max', 'true_shower_primary_id', 'true_camera_x', 'true_camera_y', 'min_true_energy_cut', 'log_reco_energy', 'reco_energy', 'gammaness', 'camera_frame_hillas_intensity_tel1', 'camera_frame_hillas_width_tel1', 'camera_frame_hillas_length_tel1', 'leakage_intensity_width_2_tel1', 'camera_frame_hillas_intensity_tel2', 'camera_frame_hillas_width_tel2', 'camera_frame_hillas_length_tel2', 'leakage_intensity_width_2_tel2']\n",
      "Missing expected columns: []\n",
      "Energy columns used: true='true_energy', reco='reco_energy'\n",
      "true_energy range [TeV]: 0.3180175721645355 → 299.97344970703125\n",
      "reco_energy range [TeV]: 0.6018515435939218 → 279.7540775007091\n",
      "Energy correlation (true vs reco): 0.9622999275398846\n"
     ]
    }
   ],
   "source": [
    "#cell 4\n",
    "from ctapipe.io import read_table\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "\n",
    "table_path = \"/dl2/event/telescope/parameters/stereo\"\n",
    "tab = read_table(first_file, table_path)\n",
    "\n",
    "print(f\"Rows: {len(tab)} | Columns: {len(tab.colnames)}\")\n",
    "print(\"First 30 columns:\", tab.colnames[:30])\n",
    "\n",
    "expected = [\"true_energy\",\"reco_energy\",\"gammaness\",\"true_alt\",\"true_az\",\"reco_alt\",\"reco_az\"]\n",
    "missing = [c for c in expected if c not in tab.colnames]\n",
    "print(\"Missing expected columns:\", missing)\n",
    "if missing:\n",
    "    print(\"⚠️ WARNING: Some expected physics columns are missing!\")\n",
    "\n",
    "def to_value_tev(col):\n",
    "    try:\n",
    "        return np.asarray(col.to_value(u.TeV), dtype=float)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return np.asarray(col.quantity.to_value(u.TeV), dtype=float)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return np.asarray(col, dtype=float)\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Resolve true/reco energy column names\n",
    "# ----------------------------------------\n",
    "\n",
    "true_name = \"true_energy\" if \"true_energy\" in tab.colnames else None\n",
    "reco_name = \"reco_energy\" if \"reco_energy\" in tab.colnames else None\n",
    "\n",
    "true_used_fallback = False\n",
    "reco_used_fallback = False\n",
    "\n",
    "# true-energy fallbacks\n",
    "for alt in [\"mc_energy\", \"sim_energy_true\"]:\n",
    "    if true_name is None and alt in tab.colnames:\n",
    "        true_name = alt\n",
    "        true_used_fallback = True\n",
    "\n",
    "# reco-energy fallbacks\n",
    "for alt in [\"energy\", \"reco_energy_mean\"]:\n",
    "    if reco_name is None and alt in tab.colnames:\n",
    "        reco_name = alt\n",
    "        reco_used_fallback = True\n",
    "\n",
    "# ----------------------------------------\n",
    "# Print selected columns and warnings\n",
    "# ----------------------------------------\n",
    "\n",
    "if true_name and reco_name:\n",
    "    if true_used_fallback:\n",
    "        print(f\"⚠️ WARNING: true_energy fallback used -> '{true_name}'\")\n",
    "    if reco_used_fallback:\n",
    "        print(f\"⚠️ WARNING: reco_energy fallback used -> '{reco_name}'\")\n",
    "\n",
    "    print(f\"Energy columns used: true='{true_name}', reco='{reco_name}'\")\n",
    "\n",
    "    te = to_value_tev(tab[true_name])\n",
    "    re = to_value_tev(tab[reco_name])\n",
    "\n",
    "    print(\"true_energy range [TeV]:\", float(np.nanmin(te)), \"→\", float(np.nanmax(te)))\n",
    "    print(\"reco_energy range [TeV]:\", float(np.nanmin(re)), \"→\", float(np.nanmax(re)))\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # Unit sanity checks\n",
    "    # ----------------------------------------\n",
    "\n",
    "    if np.nanmax(te) > 500:\n",
    "        print(\"⚠️ WARNING: true_energy appears too large → likely in GeV, not TeV.\")\n",
    "\n",
    "    if np.nanmax(re) > 500:\n",
    "        print(\"⚠️ WARNING: reco_energy appears too large → likely in GeV, not TeV.\")\n",
    "\n",
    "    if np.nanmin(te) < 1e-5:\n",
    "        print(\"⚠️ WARNING: true_energy minimum is extremely low → check units.\")\n",
    "\n",
    "    if np.nanmin(re) < 1e-5:\n",
    "        print(\"⚠️ WARNING: reco_energy minimum is extremely low → check units.\")\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # Correlation test (important physics sanity)\n",
    "    # ----------------------------------------\n",
    "\n",
    "    mask = np.isfinite(te) & np.isfinite(re)\n",
    "    if np.sum(mask) > 100:\n",
    "        corr = np.corrcoef(te[mask], re[mask])[0, 1]\n",
    "        print(\"Energy correlation (true vs reco):\", corr)\n",
    "\n",
    "        if corr < 0.4:\n",
    "            print(\"⚠️ WARNING: Very low energy correlation → reconstruction may be broken.\")\n",
    "        elif corr > 0.98:\n",
    "            print(\"⚠️ WARNING: Energy correlation too high → reco may be copying true_energy.\")\n",
    "    else:\n",
    "        print(\"⚠️ WARNING: Not enough valid energy rows for correlation test!\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ ERROR: Could not resolve energy column names.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "20d900e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column presence: {'true_alt': True, 'true_az': True, 'reco_alt': True, 'reco_az': True, 'gammaness': True}\n",
      "theta_deg p50=0.116, p90=0.401\n",
      "gammaness percentiles: {50: 0.7627079732898246, 80: 0.8948992435384067, 90: 0.9347678921632108, 95: 0.9565131945032468, 99: 0.9802342503879063}\n"
     ]
    }
   ],
   "source": [
    "#cell 5\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "\n",
    "needed = [\"true_alt\", \"true_az\", \"reco_alt\", \"reco_az\", \"gammaness\"]\n",
    "present = {c: (c in tab.colnames) for c in needed}\n",
    "\n",
    "print(\"Column presence:\", present)\n",
    "missing = [c for c in needed if not present[c]]\n",
    "if missing:\n",
    "    print(f\"⚠️ WARNING: Missing columns: {missing}\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Helpers\n",
    "# --------------------------\n",
    "\n",
    "def to_value_rad(col):\n",
    "    \"\"\"\n",
    "    Convert angle column to radians.\n",
    "\n",
    "    - If it's an astropy Quantity → convert to rad.\n",
    "    - If it's a plain float array:\n",
    "        * If max(abs(value)) > 10 → assume degrees and convert to rad.\n",
    "        * Else → assume already in radians.\n",
    "    \"\"\"\n",
    "    # Case 1: proper astropy Quantity\n",
    "    try:\n",
    "        return np.asarray(col.to_value(u.rad), dtype=float)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        return np.asarray(col.quantity.to_value(u.rad), dtype=float)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Case 2: plain floats, no units\n",
    "    arr = np.asarray(col, dtype=float)\n",
    "    if arr.size == 0:\n",
    "        return arr\n",
    "\n",
    "    max_abs = np.nanmax(np.abs(arr))\n",
    "    if max_abs > 10.0:\n",
    "        # Very likely degrees (0–90 or 0–360)\n",
    "        return np.deg2rad(arr)\n",
    "\n",
    "    # Otherwise, assume already radians\n",
    "    return arr\n",
    "\n",
    "\n",
    "\n",
    "def theta_from_altaz(true_alt, true_az, reco_alt, reco_az):\n",
    "    talt = to_value_rad(true_alt)\n",
    "    taz  = to_value_rad(true_az)\n",
    "    ralt = to_value_rad(reco_alt)\n",
    "    raz  = to_value_rad(reco_az)\n",
    "\n",
    "    cos_th = np.sin(talt)*np.sin(ralt) + np.cos(talt)*np.cos(ralt)*np.cos(taz - raz)\n",
    "    cos_th = np.clip(cos_th, -1.0, 1.0)\n",
    "    return np.arccos(cos_th)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Compute theta\n",
    "# --------------------------\n",
    "\n",
    "if all(present[c] for c in [\"true_alt\",\"true_az\",\"reco_alt\",\"reco_az\"]):\n",
    "\n",
    "    theta_deg = np.rad2deg(theta_from_altaz(\n",
    "        tab[\"true_alt\"], tab[\"true_az\"],\n",
    "        tab[\"reco_alt\"], tab[\"reco_az\"]\n",
    "    ))\n",
    "\n",
    "    p50, p90 = np.nanpercentile(theta_deg, [50, 90])\n",
    "    print(f\"theta_deg p50={p50:.3f}, p90={p90:.3f}\")\n",
    "\n",
    "    # ---- Physics sanity warnings ----\n",
    "\n",
    "    if p90 > 5:\n",
    "        print(\"⚠️ WARNING: θ90 > 5° → reconstruction likely broken or wrong units.\")\n",
    "\n",
    "    if p50 < 0.005 and p90 < 0.05:\n",
    "        print(\"⚠️ WARNING: θ values extremely small → reco_alt/az may equal true_alt/az (bug).\")\n",
    "\n",
    "    if np.nanmax(theta_deg) > 20:\n",
    "        print(\"⚠️ WARNING: Some theta values exceed 20° → unrealistic for CTA DL2.\")\n",
    "\n",
    "    if np.nanmedian(np.abs(theta_deg)) > 2:\n",
    "        print(\"⚠️ WARNING: Median angular error > 2° → reconstruction very poor.\")\n",
    "\n",
    "else:\n",
    "    print(\"Theta cannot be computed — missing angle columns.\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Gammaness summary\n",
    "# --------------------------\n",
    "\n",
    "if present[\"gammaness\"]:\n",
    "\n",
    "    gh = np.asarray(tab[\"gammaness\"], dtype=float)\n",
    "    percs = {p: float(np.nanpercentile(gh, p)) for p in (50, 80, 90, 95, 99)}\n",
    "    print(\"gammaness percentiles:\", percs)\n",
    "\n",
    "    # ---- Physics sanity warnings ----\n",
    "\n",
    "    if np.nanmin(gh) < 0 or np.nanmax(gh) > 1:\n",
    "        print(\"⚠️ WARNING: gammaness outside [0,1] → model output not normalized properly.\")\n",
    "\n",
    "    if percs[80] < 0.6:\n",
    "        print(\"⚠️ WARNING: Gammaness too low at 80th percentile → classifier likely ineffective.\")\n",
    "\n",
    "    if percs[50] > 0.8:\n",
    "        print(\"⚠️ WARNING: Gammaness distribution too high → possible bug (all events classified as gamma).\")\n",
    "\n",
    "else:\n",
    "    print(\"No 'gammaness' column to summarize.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48ca60b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HillasReconstructor_core_x', 'HillasReconstructor_core_y', 'HillasReconstructor_h_max', 'log_reco_energy', 'reco_energy', 'camera_frame_hillas_intensity_tel1', 'camera_frame_hillas_width_tel1', 'camera_frame_hillas_length_tel1', 'camera_frame_hillas_intensity_tel2', 'camera_frame_hillas_width_tel2', 'camera_frame_hillas_length_tel2', 'reco_alt', 'reco_az', 'reco_ra', 'reco_dec']\n"
     ]
    }
   ],
   "source": [
    "print([c for c in tab.colnames if \"reco\" in c.lower() or \"hillas\" in c.lower() or \"reconstruct\" in c.lower()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b9af7577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading metadata from: gamma_200_300E3GeV_20_20deg_testing_dl1_dl2.h5\n",
      "Has /simulation/service/shower_distribution: True\n",
      "\n",
      "Columns: ['obs_id', 'hist_id', 'n_entries', 'bins_energy', 'bins_core_dist', 'histogram']\n",
      "Energy bin edges (TeV): min = 0.001 , max = 1000.0\n",
      "Max simulated core distance (m): 1760.0\n",
      "Histogram entries shape: (7497,)\n",
      "NOTE: sum(n_entries) = 349860000 → NOT number of simulated showers!\n",
      "      These are HISTOGRAM weights, not MC statistics.\n",
      "No 'viewcone' stored → treated as ON-axis point-source MC.\n",
      "\n",
      "Checking run_config and config tables:\n",
      "/simulation/run_config present: False\n",
      "/simulation/config present: False\n",
      "\n",
      "=== SUMMARY ===\n",
      "✔ shower_distribution metadata present (energy/core bins OK).\n",
      "✔ Number of simulated events will be determined from DL2 rows (len(tab)).\n",
      "✔ Using spectral_index = -2.0 (not stored in metadata).\n",
      "✔ Using viewcone = 0 deg (point-source).\n"
     ]
    }
   ],
   "source": [
    "# CELL 6 — Read simulation metadata and verify integrity\n",
    "\n",
    "from ctapipe.io import read_table\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def safe_read(path):\n",
    "    \"\"\"\n",
    "    Safely attempt to read a table from the HDF5 file.\n",
    "    Returns None if the node does not exist.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return read_table(first_file, path)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "print(\"Reading metadata from:\", Path(first_file).name)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1) /simulation/service/shower_distribution \n",
    "#    This exists in sst1mpipe DL2 files and contains:\n",
    "#      - simulated energy bin edges\n",
    "#      - simulated core-distance bin edges\n",
    "#      - histogram weights (NOT number of MC events)\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "sd = safe_read(\"/simulation/service/shower_distribution\")\n",
    "print(\"Has /simulation/service/shower_distribution:\", sd is not None)\n",
    "\n",
    "if sd is not None:\n",
    "    print(\"\\nColumns:\", sd.colnames)\n",
    "\n",
    "    # ---- MC energy binning (edges in TeV) ----\n",
    "    if \"bins_energy\" in sd.colnames:\n",
    "        eb = np.asarray(sd[\"bins_energy\"][0], dtype=float)\n",
    "        print(\"Energy bin edges (TeV): min =\", np.min(eb), \", max =\", np.max(eb))\n",
    "    else:\n",
    "        print(\"⚠ WARNING: 'bins_energy' missing\")\n",
    "\n",
    "    # ---- Core-distance binning ----\n",
    "    if \"bins_core_dist\" in sd.colnames:\n",
    "        cb = np.asarray(sd[\"bins_core_dist\"][0], dtype=float)\n",
    "        print(\"Max simulated core distance (m):\", np.max(cb))\n",
    "    else:\n",
    "        print(\"⚠ WARNING: 'bins_core_dist' missing\")\n",
    "\n",
    "    # ---- Histogram counts ----\n",
    "    if \"n_entries\" in sd.colnames:\n",
    "        ne = np.asarray(sd[\"n_entries\"])\n",
    "        print(\"Histogram entries shape:\", ne.shape)\n",
    "        print(\"NOTE: sum(n_entries) =\", int(np.sum(ne)),\n",
    "              \"→ NOT number of simulated showers!\")\n",
    "        print(\"      These are HISTOGRAM weights, not MC statistics.\")\n",
    "    else:\n",
    "        print(\"⚠ WARNING: 'n_entries' missing\")\n",
    "\n",
    "    # ---- Viewcone ----\n",
    "    if \"viewcone\" in sd.colnames:\n",
    "        vc = float(sd[\"viewcone\"][0])\n",
    "        print(\"Viewcone (deg):\", vc)\n",
    "    else:\n",
    "        print(\"No 'viewcone' stored → treated as ON-axis point-source MC.\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2) Check /simulation/run_config and /simulation/config\n",
    "#    These are OPTIONAL in sst1mpipe and often missing.\n",
    "#    If they exist, they may contain:\n",
    "#      - spectral index\n",
    "#      - true pointing info\n",
    "#      - viewcone_min/max\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "print(\"\\nChecking run_config and config tables:\")\n",
    "\n",
    "for alt in (\"/simulation/run_config\", \"/simulation/config\"):\n",
    "    t = safe_read(alt)\n",
    "    print(f\"{alt} present:\", t is not None)\n",
    "\n",
    "    if t is not None:\n",
    "        print(\"Columns:\", t.colnames[:20])\n",
    "\n",
    "        try:\n",
    "            print(\"First row:\")\n",
    "            print(t[:1])\n",
    "        except Exception:\n",
    "            print(\"(Table does not support slicing)\")\n",
    "\n",
    "        print(\"NOTE: These tables do NOT contain n_showers or anything usable for MC counts.\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Summary\n",
    "# ---------------------------------------------------------------\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "\n",
    "if sd is None:\n",
    "    print(\"❌ No shower_distribution metadata found → cannot build IRFs.\")\n",
    "else:\n",
    "    print(\"✔ shower_distribution metadata present (energy/core bins OK).\")\n",
    "\n",
    "print(\"✔ Number of simulated events will be determined from DL2 rows (len(tab)).\")\n",
    "print(\"✔ Using spectral_index = -2.0 (not stored in metadata).\")\n",
    "print(\"✔ Using viewcone = 0 deg (point-source).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5e6ac27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ n_showers (from shower_distribution) = 349860000\n",
      "  DL2 events (triggered/kept)          = 393483\n",
      "  Triggered fraction ≈ 1.125e-03\n",
      "✔ Energy range (MC): 0.001 TeV → 1000.0 TeV\n",
      "✔ max_impact = 1760.0 m\n",
      "  Geometric area π·R_max² ≈ 9.731e+06 m²\n",
      "✔ spectral_index = -2.0 (default production spectrum)\n",
      "✔ viewcone = 0 deg (treated as point-source MC)\n",
      "✔ SimulatedEventsInfo kwargs: {'n_showers': 349860000, 'energy_min': <Quantity 0.001 TeV>, 'energy_max': <Quantity 1000. TeV>, 'max_impact': <Quantity 1760. m>, 'spectral_index': -2.0, 'viewcone_min': <Quantity 0. deg>, 'viewcone_max': <Quantity 0. deg>}\n",
      "\n",
      "Final SimulatedEventsInfo:\n",
      "SimulatedEventsInfo(n_showers=349860000, energy_min=0.001 TeV, energy_max=1000.00 TeV, spectral_index=-2.0, max_impact=1760.00 m, viewcone_min=0.0 degviewcone_max=0.0 deg)\n"
     ]
    }
   ],
   "source": [
    "# CELL 7 — Build SimulatedEventsInfo using proper MC statistics\n",
    "\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "import inspect\n",
    "from pyirf.simulations import SimulatedEventsInfo\n",
    "\n",
    "def build_simulated_events_info(sd, tab):\n",
    "    \"\"\"\n",
    "    Construct a SimulatedEventsInfo object using SST1M metadata:\n",
    "\n",
    "      - n_showers  from sum(sd['n_entries'])  (true MC statistics)\n",
    "      - energy_min/max from sd['bins_energy']\n",
    "      - max_impact from sd['bins_core_dist']\n",
    "      - spectral_index = -2.0 (production spectrum, not stored explicitly)\n",
    "      - viewcone = 0 deg (treated as point-source MC; diffuse angle not stored)\n",
    "\n",
    "    This matches the way ctapipe / pyirf expect simulation_info to be filled.\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------\n",
    "    # 1) Check metadata presence\n",
    "    # ------------------------------------\n",
    "    if sd is None:\n",
    "        raise RuntimeError(\n",
    "            \"ERROR: No /simulation/service/shower_distribution table. \"\n",
    "            \"Cannot build SimulatedEventsInfo.\"\n",
    "        )\n",
    "\n",
    "    # ------------------------------------\n",
    "    # 2) Number of simulated showers\n",
    "    #    -> use histogram statistics, NOT len(DL2)\n",
    "    # ------------------------------------\n",
    "    if \"n_entries\" not in sd.colnames:\n",
    "        raise RuntimeError(\n",
    "            \"ERROR: 'n_entries' missing in shower_distribution — cannot get MC statistics.\"\n",
    "        )\n",
    "\n",
    "    n_showers_hist = int(np.nansum(np.asarray(sd[\"n_entries\"])))\n",
    "    n_dl2 = len(tab)\n",
    "\n",
    "    print(f\"✔ n_showers (from shower_distribution) = {n_showers_hist}\")\n",
    "    print(f\"  DL2 events (triggered/kept)          = {n_dl2}\")\n",
    "    if n_dl2 > 0:\n",
    "        frac = n_dl2 / n_showers_hist\n",
    "        print(f\"  Triggered fraction ≈ {frac:.3e}\")\n",
    "        if frac > 0.5:\n",
    "            print(\"⚠ WARNING: Triggered fraction > 50% — check that n_entries really is MC stats.\")\n",
    "    n_showers = n_showers_hist\n",
    "\n",
    "    # ------------------------------------\n",
    "    # 3) Energy range (TeV)\n",
    "    # ------------------------------------\n",
    "    if \"bins_energy\" not in sd.colnames:\n",
    "        raise RuntimeError(\"'bins_energy' missing in shower_distribution\")\n",
    "\n",
    "    eb = np.asarray(sd[\"bins_energy\"][0], dtype=float)\n",
    "    energy_min = np.min(eb) * u.TeV\n",
    "    energy_max = np.max(eb) * u.TeV\n",
    "    print(\"✔ Energy range (MC):\", energy_min, \"→\", energy_max)\n",
    "\n",
    "    # ------------------------------------\n",
    "    # 4) Max impact parameter (m)\n",
    "    # ------------------------------------\n",
    "    if \"bins_core_dist\" not in sd.colnames:\n",
    "        raise RuntimeError(\"'bins_core_dist' missing in shower_distribution\")\n",
    "\n",
    "    cb = np.asarray(sd[\"bins_core_dist\"][0], dtype=float)\n",
    "    max_impact = np.max(cb) * u.m\n",
    "    print(\"✔ max_impact =\", max_impact)\n",
    "\n",
    "    # quick geometric sanity check\n",
    "    a_geom = np.pi * max_impact**2\n",
    "    print(f\"  Geometric area π·R_max² ≈ {a_geom.to_value(u.m**2):.3e} m²\")\n",
    "\n",
    "    # ------------------------------------\n",
    "    # 5) Spectral index\n",
    "    #    Not stored in these files → use -2.0\n",
    "    # ------------------------------------\n",
    "    spectral_index = -2.0\n",
    "    print(\"✔ spectral_index = -2.0 (default production spectrum)\")\n",
    "\n",
    "    # ------------------------------------\n",
    "    # 6) Viewcone\n",
    "    #    Not stored explicitly; treat as point-source\n",
    "    # ------------------------------------\n",
    "    viewcone_min = 0.0 * u.deg\n",
    "    viewcone_max = 0.0 * u.deg\n",
    "    print(\"✔ viewcone = 0 deg (treated as point-source MC)\")\n",
    "\n",
    "    # ------------------------------------\n",
    "    # 7) Build SimulatedEventsInfo (handle both pyirf signatures)\n",
    "    # ------------------------------------\n",
    "    sig = inspect.signature(SimulatedEventsInfo).parameters\n",
    "    params = set(sig.keys())\n",
    "\n",
    "    kwargs = dict(\n",
    "        n_showers=n_showers,\n",
    "        energy_min=energy_min,\n",
    "        energy_max=energy_max,\n",
    "        max_impact=max_impact,\n",
    "        spectral_index=spectral_index,\n",
    "    )\n",
    "\n",
    "    if {\"viewcone_min\", \"viewcone_max\"} <= params:\n",
    "        kwargs.update(viewcone_min=viewcone_min, viewcone_max=viewcone_max)\n",
    "    elif \"viewcone\" in params:\n",
    "        kwargs.update(viewcone=viewcone_max)\n",
    "    else:\n",
    "        raise RuntimeError(f\"Unexpected SimulatedEventsInfo signature: {params}\")\n",
    "\n",
    "    print(\"✔ SimulatedEventsInfo kwargs:\", kwargs)\n",
    "\n",
    "    return SimulatedEventsInfo(**kwargs)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# BUILD THE OBJECT (this runs with sd and tab loaded in previous cells)\n",
    "# --------------------------------------------------------------------\n",
    "sim_info = build_simulated_events_info(sd, tab)\n",
    "\n",
    "print(\"\\nFinal SimulatedEventsInfo:\")\n",
    "print(sim_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e55267d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 393483 true-energy values from DL2.\n",
      "MC energy bin edges: 0.001 TeV → 1000.000 TeV\n",
      "Effective energy binning range: 0.878 TeV → 283.491 TeV\n",
      "✔ Created 25 true-energy bins\n",
      "First 6 bin-edges (TeV): [0.8784 1.1058 1.3921 1.7526 2.2064 2.7777]\n",
      "Last 6 bin-edges  (TeV): [ 87.8386 110.5822 139.2148 175.261  220.6406 277.77  ]\n"
     ]
    }
   ],
   "source": [
    "# CELL 8 — Create true-energy bins (true_bins) for IRF construction\n",
    "\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from pyirf.binning import create_bins_per_decade\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Preconditions: ensure required objects exist\n",
    "# ---------------------------------------------\n",
    "if \"tab\" not in globals():\n",
    "    raise RuntimeError(\"DL2 table 'tab' not loaded — please run earlier cells.\")\n",
    "if \"sd\" not in globals():\n",
    "    raise RuntimeError(\"MC metadata 'sd' missing — please run metadata cell first.\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Helper: convert energy to TeV\n",
    "# ---------------------------------------------\n",
    "def to_value_tev(col):\n",
    "    try:\n",
    "        return np.asarray(col.to_value(u.TeV), dtype=float)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return np.asarray(col.quantity.to_value(u.TeV), dtype=float)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return np.asarray(col, dtype=float)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 1) Extract true energy values\n",
    "# ---------------------------------------------\n",
    "e_true = to_value_tev(tab[\"true_energy\"])\n",
    "print(f\"Loaded {len(e_true)} true-energy values from DL2.\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2) Retrieve MC energy bin edges\n",
    "# ---------------------------------------------\n",
    "eb = np.asarray(sd[\"bins_energy\"][0], dtype=float)\n",
    "mc_emin = np.min(eb)\n",
    "mc_emax = np.max(eb)\n",
    "print(f\"MC energy bin edges: {mc_emin:.3f} TeV → {mc_emax:.3f} TeV\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 3) Determine safe binning range\n",
    "# ---------------------------------------------\n",
    "dl2_emin = float(np.nanpercentile(e_true, 0.5))\n",
    "dl2_emax = float(np.nanpercentile(e_true, 99.5))\n",
    "\n",
    "emin = max(mc_emin, dl2_emin)\n",
    "emax = min(mc_emax, dl2_emax)\n",
    "\n",
    "# Clamp to avoid extremes\n",
    "emin = max(emin, 0.01)     # minimum 10 GeV\n",
    "emax = min(emax, 300.0)    # maximum 300 TeV (based on DL2)\n",
    "print(f\"Effective energy binning range: {emin:.3f} TeV → {emax:.3f} TeV\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4) Create bins per decade\n",
    "# ---------------------------------------------\n",
    "true_bins = create_bins_per_decade(\n",
    "    emin * u.TeV,\n",
    "    emax * u.TeV,\n",
    "    bins_per_decade=10   # ~0.1 dex resolution\n",
    ")\n",
    "\n",
    "print(f\"✔ Created {len(true_bins)-1} true-energy bins\")\n",
    "print(\"First 6 bin-edges (TeV):\", np.round(true_bins.to_value(u.TeV)[:6], 4))\n",
    "print(\"Last 6 bin-edges  (TeV):\", np.round(true_bins.to_value(u.TeV)[-6:], 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7b49dbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 393483 events for cut computation.\n",
      "Using efficiencies: gammaness=0.7, theta=0.7\n",
      "⚠ WARNING: calculate_percentile_cut failed — using manual fallback.\n",
      "⚠ WARNING: calculate_percentile_cut failed — using manual fallback.\n",
      "Median gh cut: 0.6442917522954298\n",
      "Median theta cut: 0.18606500618258706 deg\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual fallback.\n",
      "\n",
      "Selected events: 214568/393483  (54.53%)\n",
      "\n",
      "First 8 bins:\n",
      "  Bin 0: [0.8784, 1.1058)  N= 5058  kept= 2643  gh_cut=0.598  th_cut=0.255\n",
      "  Bin 1: [1.1058, 1.3921)  N=10659  kept= 5658  gh_cut=0.644  th_cut=0.215\n",
      "  Bin 2: [1.3921, 1.7526)  N=15609  kept= 8200  gh_cut=0.683  th_cut=0.190\n",
      "  Bin 3: [1.7526, 2.2064)  N=18391  kept= 9758  gh_cut=0.706  th_cut=0.171\n",
      "  Bin 4: [2.2064, 2.7777)  N=20114  kept=10695  gh_cut=0.723  th_cut=0.161\n",
      "  Bin 5: [2.7777, 3.4969)  N=21051  kept=11262  gh_cut=0.734  th_cut=0.151\n",
      "  Bin 6: [3.4969, 4.4024)  N=21383  kept=11584  gh_cut=0.734  th_cut=0.144\n",
      "  Bin 7: [4.4024, 5.5422)  N=21200  kept=11613  gh_cut=0.732  th_cut=0.141\n"
     ]
    }
   ],
   "source": [
    "# CELL 9 — Compute per-energy cuts on gammaness and theta\n",
    "\n",
    "import numpy as np\n",
    "import inspect\n",
    "import astropy.units as u\n",
    "from pyirf.cuts import calculate_percentile_cut, evaluate_binned_cut\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Preconditions\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "if \"true_bins\" not in globals():\n",
    "    raise RuntimeError(\"true_bins not defined — run Cell 8 first.\")\n",
    "\n",
    "if \"tab\" not in globals():\n",
    "    raise RuntimeError(\"DL2 table 'tab' missing — run earlier cells.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Extract DL2 columns\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "e_true = to_value_tev(tab[\"true_energy\"])\n",
    "gh     = np.asarray(tab[\"gammaness\"], dtype=float)\n",
    "\n",
    "theta  = np.rad2deg(\n",
    "            theta_from_altaz(\n",
    "                tab[\"true_alt\"], tab[\"true_az\"],\n",
    "                tab[\"reco_alt\"], tab[\"reco_az\"]\n",
    "            )\n",
    "         )\n",
    "\n",
    "edges = true_bins.to_value(u.TeV)\n",
    "\n",
    "print(f\"Loaded {len(e_true)} events for cut computation.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Target efficiencies\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "gh_eff = 0.70   # keep upper 70% gammaness (stronger gamma/hadron separation)\n",
    "th_eff = 0.70   # keep lower 70% theta (angular cut)\n",
    "\n",
    "print(f\"Using efficiencies: gammaness={gh_eff}, theta={th_eff}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helper: robust percentile cut for all pyirf versions\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def safe_percentile_cut(values, bin_values, bins, eff_keep, keep_upper=False):\n",
    "    \"\"\"\n",
    "    robust wrapper around calculate_percentile_cut\n",
    "    supports old/new pyirf API and includes fallback.\n",
    "    \"\"\"\n",
    "    sig = inspect.signature(calculate_percentile_cut)\n",
    "    target = (1 - eff_keep) if keep_upper else eff_keep\n",
    "\n",
    "    try:\n",
    "        # Newer pyirf API: uses \"efficiency\"\n",
    "        if \"efficiency\" in sig.parameters:\n",
    "            return calculate_percentile_cut(\n",
    "                values=np.asarray(values),\n",
    "                bins=np.asarray(bins),\n",
    "                bin_values=np.asarray(bin_values),\n",
    "                efficiency=target\n",
    "            )\n",
    "        # Older pyirf API: uses \"percentile\"\n",
    "        else:\n",
    "            return calculate_percentile_cut(\n",
    "                values=np.asarray(values),\n",
    "                bins=np.asarray(bins),\n",
    "                bin_values=np.asarray(bin_values),\n",
    "                percentile=target * 100.0\n",
    "            )\n",
    "\n",
    "    except Exception:\n",
    "        # Fallback manual implementation\n",
    "        print(\"⚠ WARNING: calculate_percentile_cut failed — using manual fallback.\")\n",
    "        bins = np.asarray(bins)\n",
    "        cut = np.full(len(bins) - 1, np.nan)\n",
    "\n",
    "        for i in range(len(bins) - 1):\n",
    "            m = (bin_values >= bins[i]) & (bin_values < bins[i+1])\n",
    "            if np.any(m):\n",
    "                pct = (1-eff_keep)*100 if keep_upper else eff_keep*100\n",
    "                cut[i] = np.percentile(values[m], pct)\n",
    "\n",
    "        return cut\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Compute gammaness and theta cuts\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "gh_cut = safe_percentile_cut(\n",
    "    values=gh,\n",
    "    bin_values=e_true,\n",
    "    bins=edges,\n",
    "    eff_keep=gh_eff,\n",
    "    keep_upper=True     # keep tail above threshold\n",
    ")\n",
    "\n",
    "th_cut = safe_percentile_cut(\n",
    "    values=theta,\n",
    "    bin_values=e_true,\n",
    "    bins=edges,\n",
    "    eff_keep=th_eff,\n",
    "    keep_upper=False    # keep values below threshold\n",
    ")\n",
    "\n",
    "print(\"Median gh cut:\", float(np.nanmedian(gh_cut)))\n",
    "print(\"Median theta cut:\", float(np.nanmedian(th_cut)), \"deg\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Apply the cuts\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "try:\n",
    "    sel_gh = evaluate_binned_cut(\n",
    "        values=gh, bins=edges,\n",
    "        bin_values=e_true,\n",
    "        cut=gh_cut,\n",
    "        operator=\">=\"\n",
    "    )\n",
    "    sel_th = evaluate_binned_cut(\n",
    "        values=theta, bins=edges,\n",
    "        bin_values=e_true,\n",
    "        cut=th_cut,\n",
    "        operator=\"<=\"\n",
    "    )\n",
    "except Exception:\n",
    "    print(\"⚠ WARNING: evaluate_binned_cut failed — using manual fallback.\")\n",
    "    idx = np.digitize(e_true, edges) - 1\n",
    "\n",
    "    sel_gh = np.zeros_like(gh, dtype=bool)\n",
    "    sel_th = np.zeros_like(gh, dtype=bool)\n",
    "\n",
    "    ok = (idx >= 0) & (idx < len(gh_cut))\n",
    "\n",
    "    sel_gh[ok] = gh[ok] >= gh_cut[idx[ok]]\n",
    "    sel_th[ok] = theta[ok] <= th_cut[idx[ok]]\n",
    "\n",
    "sel = sel_gh & sel_th\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cut summary\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "kept = np.count_nonzero(sel)\n",
    "total = len(sel)\n",
    "print(f\"\\nSelected events: {kept}/{total}  ({kept/total*100:.2f}%)\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Show first few bins for inspection\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "bin_ids = np.digitize(e_true, edges) - 1\n",
    "print(\"\\nFirst 8 bins:\")\n",
    "for i in range(min(8, len(edges)-1)):\n",
    "    c = np.count_nonzero(bin_ids == i)\n",
    "    k = np.count_nonzero(sel & (bin_ids == i))\n",
    "    print(f\"  Bin {i}: [{edges[i]:.4f}, {edges[i+1]:.4f})  N={c:5d}  kept={k:5d}  gh_cut={gh_cut[i]:.3f}  th_cut={th_cut[i]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7196741b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total DL2 events: 393483\n",
      "Selected events (sel=True): 214568 (54.5%)\n",
      "\n",
      "Computing Aeff BEFORE cuts...\n",
      "Computing Aeff AFTER cuts...\n",
      "\n",
      "=== Aeff summary ===\n",
      "Aeff BEFORE cuts shape: (25,)\n",
      "Aeff AFTER  cuts shape: (25,)\n",
      "Non-zero bins BEFORE cuts: 25 / 25\n",
      "Non-zero bins AFTER  cuts: 25 / 25\n",
      "\n",
      "Max Aeff BEFORE cuts: 236537523.8 m²\n",
      "Max Aeff AFTER  cuts: 129921077.2 m²\n",
      "⚠ WARNING: Aeff before cuts > 3e7 m² — suspiciously large for SST-1M.\n",
      "\n",
      "First 8 bins (Etrue, Aeff_all, Aeff_sel, eff_sel):\n",
      "  [0.878, 1.106) TeV  Aeff_all= 600855.5 m²  Aeff_sel= 313970.1 m²  eff_sel=0.523\n",
      "  [1.106, 1.392) TeV  Aeff_all=1594071.0 m²  Aeff_sel= 846163.2 m²  eff_sel=0.531\n",
      "  [1.392, 1.753) TeV  Aeff_all=2938774.5 m²  Aeff_sel=1543849.8 m²  eff_sel=0.525\n",
      "  [1.753, 2.206) TeV  Aeff_all=4359097.0 m²  Aeff_sel=2312874.1 m²  eff_sel=0.531\n",
      "  [2.206, 2.778) TeV  Aeff_all=6001912.1 m²  Aeff_sel=3191331.9 m²  eff_sel=0.532\n",
      "  [2.778, 3.497) TeV  Aeff_all=7907950.1 m²  Aeff_sel=4230646.2 m²  eff_sel=0.535\n",
      "  [3.497, 4.402) TeV  Aeff_all=10112530.0 m²  Aeff_sel=5478349.5 m²  eff_sel=0.542\n",
      "  [4.402, 5.542) TeV  Aeff_all=12621967.2 m²  Aeff_sel=6914099.3 m²  eff_sel=0.548\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# CELL 10 — Effective area before and after cuts\n",
    "\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from astropy.table import Table\n",
    "from pyirf.irf import effective_area_per_energy\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Preconditions\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "if \"sim_info\" not in globals():\n",
    "    raise RuntimeError(\"sim_info missing — run Cell 7.\")\n",
    "if \"true_bins\" not in globals():\n",
    "    raise RuntimeError(\"true_bins missing — run Cell 8.\")\n",
    "if \"sel\" not in globals():\n",
    "    raise RuntimeError(\"sel (selection mask) missing — run Cell 9.\")\n",
    "if \"tab\" not in globals():\n",
    "    raise RuntimeError(\"DL2 table 'tab' missing — run earlier cells.\")\n",
    "\n",
    "# Helper (if not already available in this scope)\n",
    "def to_value_tev(col):\n",
    "    try:\n",
    "        return np.asarray(col.to_value(u.TeV), dtype=float)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return np.asarray(col.quantity.to_value(u.TeV), dtype=float)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return np.asarray(col, dtype=float)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Extract true energies\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "true_e = to_value_tev(tab[\"true_energy\"])\n",
    "\n",
    "if np.any(~np.isfinite(true_e)):\n",
    "    print(\"⚠ WARNING: Non-finite values found in true_energy; they will be ignored in histograms.\")\n",
    "\n",
    "print(f\"Total DL2 events: {len(true_e)}\")\n",
    "print(f\"Selected events (sel=True): {np.count_nonzero(sel)} \"\n",
    "      f\"({np.count_nonzero(sel)/len(true_e)*100:.1f}%)\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Build pyirf tables: before and after cuts\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "t_all = Table()\n",
    "t_all[\"true_energy\"] = true_e * u.TeV\n",
    "\n",
    "t_sel = Table()\n",
    "t_sel[\"true_energy\"] = true_e[sel] * u.TeV\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Compute effective area\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"\\nComputing Aeff BEFORE cuts...\")\n",
    "aeff_all = effective_area_per_energy(\n",
    "    selected_events=t_all,\n",
    "    simulation_info=sim_info,\n",
    "    true_energy_bins=true_bins,\n",
    ").to_value(u.m**2)\n",
    "\n",
    "print(\"Computing Aeff AFTER cuts...\")\n",
    "aeff_sel = effective_area_per_energy(\n",
    "    selected_events=t_sel,\n",
    "    simulation_info=sim_info,\n",
    "    true_energy_bins=true_bins,\n",
    ").to_value(u.m**2)\n",
    "\n",
    "edges_TeV = true_bins.to_value(u.TeV)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Sanity checks and summary\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"\\n=== Aeff summary ===\")\n",
    "print(\"Aeff BEFORE cuts shape:\", aeff_all.shape)\n",
    "print(\"Aeff AFTER  cuts shape:\", aeff_sel.shape)\n",
    "\n",
    "print(\"Non-zero bins BEFORE cuts:\", int(np.count_nonzero(aeff_all)), \"/\", len(aeff_all))\n",
    "print(\"Non-zero bins AFTER  cuts:\", int(np.count_nonzero(aeff_sel)), \"/\", len(aeff_sel))\n",
    "\n",
    "max_all = float(np.nanmax(aeff_all))\n",
    "max_sel = float(np.nanmax(aeff_sel))\n",
    "\n",
    "print(\"\\nMax Aeff BEFORE cuts: {:.1f} m²\".format(max_all))\n",
    "print(\"Max Aeff AFTER  cuts: {:.1f} m²\".format(max_sel))\n",
    "\n",
    "if max_all < 10:\n",
    "    print(\"⚠ WARNING: Aeff before cuts extremely small (<10 m²) — check n_showers or impact range.\")\n",
    "if max_all > 3e7:\n",
    "    print(\"⚠ WARNING: Aeff before cuts > 3e7 m² — suspiciously large for SST-1M.\")\n",
    "\n",
    "if max_sel < 1:\n",
    "    print(\"⚠ WARNING: Aeff after cuts extremely small (<1 m²) — cuts might be too strong.\")\n",
    "if max_sel > max_all * 1.01:\n",
    "    print(\"⚠ WARNING: Aeff after cuts exceeds Aeff before cuts somewhere — something is inconsistent.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Print first few bins for inspection\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"\\nFirst 8 bins (Etrue, Aeff_all, Aeff_sel, eff_sel):\")\n",
    "for i in range(min(8, len(aeff_all))):\n",
    "    e_min = edges_TeV[i]\n",
    "    e_max = edges_TeV[i+1]\n",
    "    aa = aeff_all[i]\n",
    "    asel = aeff_sel[i]\n",
    "    if aa > 0:\n",
    "        eff = asel / aa\n",
    "    else:\n",
    "        eff = np.nan\n",
    "    print(f\"  [{e_min:.3f}, {e_max:.3f}) TeV  \"\n",
    "          f\"Aeff_all={aa:9.1f} m²  Aeff_sel={asel:9.1f} m²  eff_sel={eff:5.3f}\")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "361379d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma file: gamma_200_300E3GeV_20_20deg_testing_dl1_dl2.h5\n",
      "Zenith token: _20_20deg_\n",
      "Found 1 proton files\n",
      "Examples: ['proton_400_500E3GeV_20_20deg_testing_dl1_dl2.h5']\n",
      "\n",
      "Using proton file: proton_400_500E3GeV_20_20deg_testing_dl1_dl2.h5\n",
      "Proton parameter tables: ['/dl2/event/telescope/parameters/stereo']\n",
      "Selected proton table: /dl2/event/telescope/parameters/stereo\n",
      "Proton rows: 2754879 | Columns: 35\n",
      "Missing expected proton columns: []\n",
      "Proton reco_energy range [TeV]: 0.5921603736083508 → 284.53092990263553\n",
      "\n",
      "Proton DL2 loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# CELL — Load matching proton DL2 file (for background estimation)\n",
    "# ================================================================\n",
    "\n",
    "import re, glob, h5py\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from ctapipe.io import read_table\n",
    "\n",
    "print(\"Gamma file:\", Path(first_file).name)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 1) Extract zenith/azimuth token from gamma filename\n",
    "#    (e.g. _20_20deg_)\n",
    "# ----------------------------------------------------------\n",
    "name = Path(first_file).name\n",
    "m = re.search(r\"_[0-9]{1,2}_[0-9]{1,2}deg_\", name)\n",
    "zen_token = m.group(0) if m else \"\"\n",
    "print(\"Zenith token:\", zen_token or \"(none found)\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 2) Find proton files matching the same zenith configuration\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "if zen_token:\n",
    "    proton_files = sorted(glob.glob(str(Path(DL2_DIR) / f\"proton*{zen_token}*.h5\")))\n",
    "else:\n",
    "    proton_files = []\n",
    "\n",
    "# Fallback: any proton file\n",
    "if not proton_files:\n",
    "    print(\"⚠ WARNING: No zenith-matched proton files found — using all proton_*.h5\")\n",
    "    proton_files = sorted(glob.glob(str(Path(DL2_DIR) / \"proton_*.h5\")))\n",
    "\n",
    "print(f\"Found {len(proton_files)} proton files\")\n",
    "print(\"Examples:\", [Path(f).name for f in proton_files[:5]])\n",
    "\n",
    "assert proton_files, \"❌ ERROR: No proton files available.\"\n",
    "\n",
    "# Use the first matching proton file\n",
    "pfile = proton_files[0]\n",
    "print(\"\\nUsing proton file:\", Path(pfile).name)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 3) List DL2 parameter tables in proton file\n",
    "# ----------------------------------------------------------\n",
    "def list_parameter_tables(h5file):\n",
    "    out = []\n",
    "    with h5py.File(h5file, \"r\") as h5:\n",
    "        base = \"/dl2/event/telescope/parameters\"\n",
    "        if base in h5:\n",
    "            out = [f\"{base}/{k}\" for k in h5[base].keys()]\n",
    "    return out\n",
    "\n",
    "pcands = list_parameter_tables(pfile)\n",
    "print(\"Proton parameter tables:\", pcands)\n",
    "\n",
    "# Prefer stereo reconstruction if available\n",
    "ptable = next((p for p in pcands if p.endswith(\"/stereo\")),\n",
    "              (pcands[0] if pcands else None))\n",
    "\n",
    "assert ptable, \"❌ ERROR: No proton parameter table found.\"\n",
    "print(\"Selected proton table:\", ptable)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 4) Load proton DL2 table\n",
    "# ----------------------------------------------------------\n",
    "ptab = read_table(pfile, ptable)\n",
    "print(f\"Proton rows: {len(ptab)} | Columns: {len(ptab.colnames)}\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 5) Check required physics columns\n",
    "# ----------------------------------------------------------\n",
    "needed = [\"reco_energy\", \"gammaness\", \"true_alt\", \"true_az\", \"reco_alt\", \"reco_az\"]\n",
    "missing = [c for c in needed if c not in ptab.colnames]\n",
    "\n",
    "print(\"Missing expected proton columns:\", missing)\n",
    "if missing:\n",
    "    print(\"⚠ WARNING: Proton DL2 file missing necessary columns; \"\n",
    "          \"background estimation may be incomplete.\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 6) Sanity: convert reco_energy to TeV, same logic as gamma cell\n",
    "# ----------------------------------------------------------\n",
    "def to_value_tev(col):\n",
    "    try:\n",
    "        return np.asarray(col.to_value(u.TeV), dtype=float)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return np.asarray(col.quantity.to_value(u.TeV), dtype=float)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return np.asarray(col, dtype=float)\n",
    "\n",
    "if \"reco_energy\" in ptab.colnames:\n",
    "    ereco = to_value_tev(ptab[\"reco_energy\"])\n",
    "    print(\"Proton reco_energy range [TeV]:\",\n",
    "          float(np.nanmin(ereco)), \"→\", float(np.nanmax(ereco)))\n",
    "\n",
    "    # unit sanity check\n",
    "    if np.nanmax(ereco) > 500:\n",
    "        print(\"⚠ WARNING: Proton reco_energy > 500 TeV — values likely in GeV, \"\n",
    "              \"check units or conversion.\")\n",
    "else:\n",
    "    print(\"❌ ERROR: No 'reco_energy' column in proton table.\")\n",
    "    ereco = None\n",
    "\n",
    "print(\"\\nProton DL2 loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ea8e0ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ WARNING: evaluate_binned_cut failed for protons — using manual per-bin fallback.\n",
      "Proton selected: 158 / 2754879  (0.01%)\n",
      "\n",
      "Reco-energy bins: 13\n",
      "Nonzero rate bins: 13\n",
      "First 10 bins summary (Ereco, N_sel, rate):\n",
      "  [1.201, 1.602) TeV  N_sel=      1  rate=0.196078 Hz\n",
      "  [1.602, 2.136) TeV  N_sel=      5  rate=0.980392 Hz\n",
      "  [2.136, 2.848) TeV  N_sel=      4  rate=0.784314 Hz\n",
      "  [2.848, 3.798) TeV  N_sel=      6  rate=1.176471 Hz\n",
      "  [3.798, 5.065) TeV  N_sel=      3  rate=0.588235 Hz\n",
      "  [5.065, 6.754) TeV  N_sel=      9  rate=1.764706 Hz\n",
      "  [6.754, 9.007) TeV  N_sel=     11  rate=2.156863 Hz\n",
      "  [9.007, 12.011) TeV  N_sel=     14  rate=2.745098 Hz\n",
      "  [12.011, 16.016) TeV  N_sel=     10  rate=1.960784 Hz\n",
      "  [16.016, 21.358) TeV  N_sel=      8  rate=1.568627 Hz\n",
      "\n",
      "Display cuts → Gammaness_cut=0.644, Theta_cut_deg=0.186\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL — Apply gamma cuts to protons & build background shape\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from pyirf.cuts import evaluate_binned_cut\n",
    "from pyirf.binning import create_bins_per_decade\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Preconditions\n",
    "# ------------------------------------------------------------\n",
    "if \"ptab\" not in globals():\n",
    "    raise RuntimeError(\"Proton DL2 table 'ptab' not loaded — run proton loader cell first.\")\n",
    "if \"tab\" not in globals():\n",
    "    raise RuntimeError(\"Gamma DL2 table 'tab' not loaded.\")\n",
    "if \"true_bins\" not in globals():\n",
    "    raise RuntimeError(\"true_bins not defined — run energy-binning cell first.\")\n",
    "if \"gh_cut\" not in globals() or \"th_cut\" not in globals():\n",
    "    raise RuntimeError(\"gh_cut / th_cut not defined — run gamma cut cell first.\")\n",
    "\n",
    "# Use the same true-energy bin edges as for gamma cuts\n",
    "e_true_edges_TeV = true_bins.to_value(u.TeV)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helper functions (same style as earlier cells)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def to_value_rad(col):\n",
    "    try:\n",
    "        return np.asarray(col.to_value(u.rad))\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return np.asarray(col.quantity.to_value(u.rad))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return np.asarray(col)\n",
    "\n",
    "def to_value_tev(col):\n",
    "    try:\n",
    "        return np.asarray(col.to_value(u.TeV), dtype=float)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return np.asarray(col.quantity.to_value(u.TeV), dtype=float)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return np.asarray(col, dtype=float)\n",
    "\n",
    "def theta_from_altaz(true_alt, true_az, reco_alt, reco_az):\n",
    "    talt = to_value_rad(true_alt); taz = to_value_rad(true_az)\n",
    "    ralt = to_value_rad(reco_alt); raz = to_value_rad(reco_az)\n",
    "    cos_th = (\n",
    "        np.sin(talt) * np.sin(ralt)\n",
    "        + np.cos(talt) * np.cos(ralt) * np.cos(taz - raz)\n",
    "    )\n",
    "    cos_th = np.clip(cos_th, -1.0, 1.0)\n",
    "    return np.arccos(cos_th)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Apply gamma-derived cuts to proton events (per TRUE-E bin)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "p_true = to_value_tev(ptab[\"true_energy\"])\n",
    "p_gh   = np.asarray(ptab[\"gammaness\"], dtype=float)\n",
    "p_th   = np.rad2deg(\n",
    "    theta_from_altaz(\n",
    "        ptab[\"true_alt\"], ptab[\"true_az\"],\n",
    "        ptab[\"reco_alt\"], ptab[\"reco_az\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "try:\n",
    "    p_sel_gh = evaluate_binned_cut(\n",
    "        values=p_gh,\n",
    "        bins=e_true_edges_TeV,\n",
    "        bin_values=p_true,\n",
    "        cut=gh_cut,\n",
    "        operator=\">=\"\n",
    "    )\n",
    "    p_sel_th = evaluate_binned_cut(\n",
    "        values=p_th,\n",
    "        bins=e_true_edges_TeV,\n",
    "        bin_values=p_true,\n",
    "        cut=th_cut,\n",
    "        operator=\"<=\"\n",
    "    )\n",
    "except Exception:\n",
    "    print(\"⚠ WARNING: evaluate_binned_cut failed for protons — using manual per-bin fallback.\")\n",
    "    idx = np.digitize(p_true, e_true_edges_TeV) - 1\n",
    "    p_sel_gh = np.zeros_like(p_gh, dtype=bool)\n",
    "    p_sel_th = np.zeros_like(p_gh, dtype=bool)\n",
    "\n",
    "    good = (idx >= 0) & (idx < len(gh_cut))\n",
    "    p_sel_gh[good] = p_gh[good] >= gh_cut[idx[good]]\n",
    "    p_sel_th[good] = p_th[good] <= th_cut[idx[good]]\n",
    "\n",
    "p_sel = p_sel_gh & p_sel_th\n",
    "\n",
    "n_p_sel = int(np.count_nonzero(p_sel))\n",
    "n_p_tot = len(p_sel)\n",
    "print(f\"Proton selected: {n_p_sel} / {n_p_tot}  ({100*n_p_sel/n_p_tot:.2f}%)\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Build BACKGROUND SHAPE over RECO-energy bins\n",
    "#    NOTE: We fix only the SHAPE here; absolute normalization\n",
    "#          is controlled by TOTAL_RATE_TARGET_HZ (placeholder).\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "p_reco = to_value_tev(ptab[\"reco_energy\"])\n",
    "\n",
    "# \"Safe\" reco-energy range from percentiles\n",
    "pr_emin = max(0.05, float(np.nanpercentile(p_reco, 0.5)))\n",
    "pr_emax = min(60.0, float(np.nanpercentile(p_reco, 99.5)))\n",
    "\n",
    "e_reco_bins = create_bins_per_decade(\n",
    "    (pr_emin * u.TeV),\n",
    "    (pr_emax * u.TeV),\n",
    "    bins_per_decade=8,\n",
    ").to_value(u.TeV)\n",
    "\n",
    "counts_sel, _ = np.histogram(p_reco[p_sel], bins=e_reco_bins)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Set absolute background scale (placeholder)\n",
    "# ------------------------------------------------------------\n",
    "# This normalizes the spectrum so that the SUM over all bins is\n",
    "# TOTAL_RATE_TARGET_HZ. The SHAPE is correct from MC; the scale\n",
    "# is arbitrary here and should be tuned to a realistic rate\n",
    "# (e.g. from data, or from a known background model).\n",
    "TOTAL_RATE_TARGET_HZ = 20.0\n",
    "\n",
    "if counts_sel.sum() == 0:\n",
    "    print(\"⚠ WARNING: No selected proton events — background rates set to ~0.\")\n",
    "    rates = np.full(len(e_reco_bins) - 1, 1e-12)\n",
    "else:\n",
    "    rates = TOTAL_RATE_TARGET_HZ * counts_sel.astype(float) / counts_sel.sum()\n",
    "    rates = np.maximum(rates, 1e-12)\n",
    "\n",
    "print(f\"\\nReco-energy bins: {len(e_reco_bins)-1}\")\n",
    "print(\"Nonzero rate bins:\", int(np.count_nonzero(rates)))\n",
    "print(\"First 10 bins summary (Ereco, N_sel, rate):\")\n",
    "for i in range(min(10, len(rates))):\n",
    "    print(\n",
    "        f\"  [{e_reco_bins[i]:.3f}, {e_reco_bins[i+1]:.3f}) TeV  \"\n",
    "        f\"N_sel={counts_sel[i]:7d}  rate={rates[i]:.6f} Hz\"\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Display \"representative\" cuts (for CSV metadata)\n",
    "# ------------------------------------------------------------\n",
    "gh_display = float(np.nanmedian(gh_cut))\n",
    "th_display = float(np.nanmedian(th_cut))\n",
    "print(f\"\\nDisplay cuts → Gammaness_cut={gh_display:.3f}, Theta_cut_deg={th_display:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24728184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zenith angle (deg): 20\n",
      "Saved gamma CSV: SST1M_csv\\SST1M_gamma_irf_gheffi_0.70_theffi_0.70.csv\n",
      "Saved backg CSV: SST1M_csv\\SST1M_backg_irf_gheffi_0.70_theffi_0.70.csv\n",
      "Gamma head:\n",
      "    ZD_deg  Etrue_min_TeV  Etrue_max_TeV       Aeff_m2  emig_mu_loc  \\\n",
      "0    20.0       0.988613       1.244591  5.523159e+05     0.032257   \n",
      "1    20.0       1.244591       1.566847  1.168805e+06     0.001696   \n",
      "2    20.0       1.566847       1.972543  1.915582e+06    -0.007345   \n",
      "\n",
      "   emig_mu_scale  emig_mu_a      emig_model  \n",
      "0       0.100580    1.98594  log10_skewnorm  \n",
      "1       0.090290    1.54035  log10_skewnorm  \n",
      "2       0.040725        NaN     log10_moyal  \n",
      "Backg head:\n",
      "    ZD_deg  Ereco_min_TeV  Ereco_max_TeV  BckgRate_per_second  Theta_cut_deg  \\\n",
      "0    20.0       1.201065       1.601645             0.904491       13.27004   \n",
      "1    20.0       1.601645       2.135828             1.590875       13.27004   \n",
      "2    20.0       2.135828       2.848173             1.655004       13.27004   \n",
      "\n",
      "   Gammaness_cut  \n",
      "0       0.697983  \n",
      "1       0.697983  \n",
      "2       0.697983  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import astropy.units as u\n",
    "\n",
    "# --- required inputs: first_file, true_bins, aeff_sel, mu_loc, mu_scale, mu_a, model\n",
    "#                      ptab, e_reco_bins, rates, gh_display, th_display\n",
    "\n",
    "# ------------------------------\n",
    "# 1) Extract zenith angle\n",
    "# ------------------------------\n",
    "name = Path(first_file).name\n",
    "m = re.search(r\"_([0-9]{1,2})_([0-9]{1,2})deg_\", name)\n",
    "zenith = int(m.group(1)) if m else -1\n",
    "print(\"Zenith angle (deg):\", zenith)\n",
    "\n",
    "e_true_edges_TeV = true_bins.to_value(u.TeV)\n",
    "\n",
    "# ------------------------------\n",
    "# Migration model mapping\n",
    "# ------------------------------\n",
    "def map_model(x):\n",
    "    if x is None:\n",
    "        return \"Gaus\"\n",
    "    x = str(x).lower()\n",
    "    if \"log\" in x:\n",
    "        return \"LogNorm\"\n",
    "    if \"gau\" in x or \"norm\" in x:\n",
    "        return \"Gaus\"\n",
    "    return \"Gaus\"  # fallback\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 2) Build gamma IRF CSV\n",
    "# ------------------------------\n",
    "gamma_rows = []\n",
    "for i in range(len(e_true_edges_TeV) - 1):\n",
    "\n",
    "    loc  = mu_loc[i]   if np.isfinite(mu_loc[i])   else 0.0\n",
    "    scale= mu_scale[i] if np.isfinite(mu_scale[i]) else 0.3\n",
    "    aparam = mu_a[i]   if np.isfinite(mu_a[i])     else 2.0\n",
    "    model_label = map_model(model[i])\n",
    "\n",
    "    aeff_val = max(0.0, float(aeff_sel[i]))\n",
    "\n",
    "    row = {\n",
    "        \"SPECIES\": \"gamma\",\n",
    "        \"ZD_deg\":  float(zenith),\n",
    "        \"Etrue_min_TeV\": float(e_true_edges_TeV[i]),\n",
    "        \"Etrue_max_TeV\": float(e_true_edges_TeV[i+1]),\n",
    "        \"Aeff_m2\": aeff_val,\n",
    "        \"emig_mu_loc\": float(loc),\n",
    "        \"emig_mu_scale\": float(scale),\n",
    "        \"emig_mu_a\": float(aparam),\n",
    "        \"emig_model\": model_label,\n",
    "    }\n",
    "    gamma_rows.append(row)\n",
    "\n",
    "df_gamma = pd.DataFrame(gamma_rows)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 3) Build background IRF CSV\n",
    "# ------------------------------\n",
    "back_rows = []\n",
    "for i in range(len(e_reco_bins) - 1):\n",
    "    row = {\n",
    "        \"SPECIES\": \"proton\",\n",
    "        \"ZD_deg\": float(zenith),\n",
    "        \"Ereco_min_TeV\": float(e_reco_bins[i]),\n",
    "        \"Ereco_max_TeV\": float(e_reco_bins[i+1]),\n",
    "        \"BckgRate_per_second\": float(rates[i]),\n",
    "        \"Theta_cut_deg\": float(th_display),\n",
    "        \"Gammaness_cut\": float(gh_display),\n",
    "    }\n",
    "    back_rows.append(row)\n",
    "\n",
    "df_back = pd.DataFrame(back_rows)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 4) Save\n",
    "# ------------------------------\n",
    "OUTPUT_DIR = Path(\"./SST1M_csv\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tag = f\"gh70_th70\"\n",
    "\n",
    "gamma_csv = OUTPUT_DIR / f\"SST1M_gamma_ZD{zenith}_{tag}.csv\"\n",
    "backg_csv = OUTPUT_DIR / f\"SST1M_backg_ZD{zenith}_{tag}.csv\"\n",
    "\n",
    "df_gamma.to_csv(gamma_csv, index=False)\n",
    "df_back.to_csv(backg_csv, index=False)\n",
    "\n",
    "print(\"Saved gamma CSV:\", gamma_csv)\n",
    "print(\"Saved backg CSV:\", backg_csv)\n",
    "\n",
    "print(\"\\nGamma head:\\n\", df_gamma.head(3))\n",
    "print(\"\\nBackground head:\\n\", df_back.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece2dc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "[zd=20 eff=0.40] rows: back=14, gamma=30\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "[zd=30 eff=0.40] rows: back=13, gamma=30\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a.nakhle\\Desktop\\sst1mpipe\\.conda\\Lib\\site-packages\\pyirf\\irf\\effective_area.py:29: RuntimeWarning: invalid value encountered in divide\n",
      "  return (n_selected / n_simulated) * area\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[zd=40 eff=0.40] rows: back=12, gamma=30\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a.nakhle\\Desktop\\sst1mpipe\\.conda\\Lib\\site-packages\\pyirf\\irf\\effective_area.py:29: RuntimeWarning: invalid value encountered in divide\n",
      "  return (n_selected / n_simulated) * area\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[zd=60 eff=0.40] rows: back=7, gamma=30\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "[zd=20 eff=0.70] rows: back=14, gamma=30\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "[zd=30 eff=0.70] rows: back=13, gamma=30\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a.nakhle\\Desktop\\sst1mpipe\\.conda\\Lib\\site-packages\\pyirf\\irf\\effective_area.py:29: RuntimeWarning: invalid value encountered in divide\n",
      "  return (n_selected / n_simulated) * area\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[zd=40 eff=0.70] rows: back=12, gamma=30\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a.nakhle\\Desktop\\sst1mpipe\\.conda\\Lib\\site-packages\\pyirf\\irf\\effective_area.py:29: RuntimeWarning: invalid value encountered in divide\n",
      "  return (n_selected / n_simulated) * area\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[zd=60 eff=0.70] rows: back=7, gamma=30\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "[zd=20 eff=0.90] rows: back=14, gamma=30\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n",
      "⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\n"
     ]
    }
   ],
   "source": [
    "# ONE-CELL PIPELINE: build IRF CSVs for all zeniths using gamma + gamma_point (signal) and proton (background)\n",
    "# Efficiencies: 0.40, 0.70, 0.90\n",
    "# Output: ./final_CSV/SST1M_gamma_irf_gheffi_XX_theffi_XX.csv and SST1M_backg_irf_gheffi_XX_theffi_XX.csv\n",
    "\n",
    "import re, glob, inspect\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import astropy.units as u\n",
    "from astropy.table import vstack, Table\n",
    "import h5py\n",
    "\n",
    "from ctapipe.io import read_table\n",
    "\n",
    "from pyirf.cuts import calculate_percentile_cut, evaluate_binned_cut\n",
    "from pyirf.binning import create_bins_per_decade\n",
    "from pyirf.irf import effective_area_per_energy\n",
    "from pyirf.simulations import SimulatedEventsInfo\n",
    "\n",
    "from scipy.stats import moyal, skewnorm  # import once, not inside the loop\n",
    "\n",
    "# ---------------- utilities ----------------\n",
    "\n",
    "def zd_from_filename(path: str):\n",
    "    \"\"\"Extract zenith angle in deg from filename like *_20_20deg_*.h5\"\"\"\n",
    "    m = re.search(r\"_([0-9]{1,2})_([0-9]{1,2})deg_\", Path(path).name)\n",
    "    if not m:\n",
    "        return None\n",
    "    return float(int(m.group(1)))\n",
    "\n",
    "\n",
    "def to_value_tev(col):\n",
    "    \"\"\"Robust conversion of a column to float array in TeV.\"\"\"\n",
    "    for attr in (\"to_value\", \"quantity\"):\n",
    "        try:\n",
    "            if attr == \"to_value\":\n",
    "                return np.asarray(col.to_value(u.TeV), dtype=float)\n",
    "            return np.asarray(col.quantity.to_value(u.TeV), dtype=float)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return np.asarray(col, dtype=float)\n",
    "\n",
    "\n",
    "def to_value_rad(col):\n",
    "    \"\"\"Robust conversion of an angle column to radians (float array).\"\"\"\n",
    "    for attr in (\"to_value\", \"quantity\"):\n",
    "        try:\n",
    "            if attr == \"to_value\":\n",
    "                return np.asarray(col.to_value(u.rad))\n",
    "            return np.asarray(col.quantity.to_value(u.rad))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return np.asarray(col)\n",
    "\n",
    "\n",
    "def theta_from_altaz(true_alt, true_az, reco_alt, reco_az):\n",
    "    \"\"\"Angular separation between true and reconstructed directions, using alt/az.\"\"\"\n",
    "    talt = to_value_rad(true_alt)\n",
    "    taz  = to_value_rad(true_az)\n",
    "    ralt = to_value_rad(reco_alt)\n",
    "    raz  = to_value_rad(reco_az)\n",
    "    cos_th = (\n",
    "        np.sin(talt) * np.sin(ralt)\n",
    "        + np.cos(talt) * np.cos(ralt) * np.cos(taz - raz)\n",
    "    )\n",
    "    return np.arccos(np.clip(cos_th, -1.0, 1.0))\n",
    "\n",
    "\n",
    "def read_param_table(path: str):\n",
    "    \"\"\"\n",
    "    Load a DL2 parameter table from a HDF5 DL2 file.\n",
    "    Prefer /dl2/event/telescope/parameters/stereo if present,\n",
    "    otherwise use the first available parameter table.\n",
    "    \"\"\"\n",
    "    with h5py.File(path, \"r\") as h5:\n",
    "        base = \"/dl2/event/telescope/parameters\"\n",
    "        if base not in h5:\n",
    "            raise KeyError(f\"{base} missing in {path}\")\n",
    "\n",
    "        # prefer stereo\n",
    "        candidates = [f\"{base}/stereo\"] + [\n",
    "            f\"{base}/{k}\" for k in h5[base].keys() if k != \"stereo\"\n",
    "        ]\n",
    "        for p in candidates:\n",
    "            if p in h5:\n",
    "                return read_table(path, p)\n",
    "\n",
    "    raise KeyError(\"No parameters table found\")\n",
    "\n",
    "\n",
    "def stack_tables(paths):\n",
    "    \"\"\"\n",
    "    Read and vertically stack multiple DL2 parameter tables into one astropy Table.\n",
    "    Metadata is cleared to avoid conflicts.\n",
    "    \"\"\"\n",
    "    tabs = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            t = read_param_table(p)\n",
    "            t.meta = {}  # drop metadata to avoid conflicts in vstack\n",
    "            tabs.append(t)\n",
    "        except Exception as e:\n",
    "            print(f\"[skip] {Path(p).name}: {e}\")\n",
    "    if not tabs:\n",
    "        return None\n",
    "    return vstack(tabs, metadata_conflicts=\"silent\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def as_float_array(x):\n",
    "    \"\"\"\n",
    "    Convert the output of calculate_percentile_cut or similar\n",
    "    to a simple 1D float numpy array, regardless of structure.\n",
    "    \"\"\"\n",
    "    if hasattr(x, \"colnames\"):\n",
    "        name = \"cut\" if \"cut\" in x.colnames else x.colnames[0]\n",
    "        col = x[name]\n",
    "        try:\n",
    "            return np.asarray(col.to_value(u.one), dtype=float)\n",
    "        except Exception:\n",
    "            return np.asarray(col, dtype=float)\n",
    "\n",
    "    if isinstance(x, np.ndarray) and x.dtype.names:\n",
    "        fld = \"cut\" if \"cut\" in x.dtype.names else x.dtype.names[0]\n",
    "        return np.asarray(x[fld], dtype=float)\n",
    "\n",
    "    try:\n",
    "        return np.asarray(x.to_value(u.one), dtype=float)\n",
    "    except Exception:\n",
    "        return np.asarray(x, dtype=float)\n",
    "\n",
    "\n",
    "def percentile_cut_compat(values, bin_values, edges, eff_keep, upper_tail=False):\n",
    "    \"\"\"\n",
    "    Wrapper around pyirf.calculate_percentile_cut with:\n",
    "      - compatibility for both 'efficiency'/'percentile' signatures\n",
    "      - manual per-bin percentile fallback if it crashes\n",
    "    \"\"\"\n",
    "    values = np.asarray(values, float)\n",
    "    bin_values = np.asarray(bin_values, float)\n",
    "    edges = np.asarray(edges, float)\n",
    "\n",
    "    sig = inspect.signature(calculate_percentile_cut)\n",
    "    params = set(sig.parameters.keys())\n",
    "    kwargs = dict(\n",
    "        values=values,\n",
    "        bins=edges,\n",
    "        bin_values=bin_values,\n",
    "    )\n",
    "\n",
    "    target = (1.0 - eff_keep) if upper_tail else eff_keep\n",
    "\n",
    "    if \"efficiency\" in params:\n",
    "        kwargs[\"efficiency\"] = target\n",
    "    else:\n",
    "        kwargs[\"percentile\"] = target * 100.0\n",
    "\n",
    "    if \"fill_value\" in params:\n",
    "        kwargs[\"fill_value\"] = np.nan\n",
    "\n",
    "    try:\n",
    "        out = calculate_percentile_cut(**kwargs)\n",
    "        return as_float_array(out)\n",
    "    except Exception:\n",
    "        # manual fallback: per-bin percentile\n",
    "        print(\"⚠ WARNING: calculate_percentile_cut failed — using manual per-bin percentile fallback.\")\n",
    "        cut = np.full(len(edges) - 1, np.nan)\n",
    "        pct = (1.0 - eff_keep) * 100.0 if upper_tail else eff_keep * 100.0\n",
    "\n",
    "        for i in range(len(edges) - 1):\n",
    "            m = (bin_values >= edges[i]) & (bin_values < edges[i + 1]) & np.isfinite(values)\n",
    "            if np.any(m):\n",
    "                cut[i] = np.nanpercentile(values[m], pct)\n",
    "\n",
    "        return cut\n",
    "\n",
    "\n",
    "def eval_binned(values, bin_values, edges, cut, op):\n",
    "    \"\"\"\n",
    "    Wrapper around pyirf.evaluate_binned_cut with robust manual fallback.\n",
    "    \"\"\"\n",
    "    values = np.asarray(values, float)\n",
    "    bin_values = np.asarray(bin_values, float)\n",
    "    edges = np.asarray(edges, float)\n",
    "    cut_arr = as_float_array(cut)\n",
    "\n",
    "    try:\n",
    "        return evaluate_binned_cut(\n",
    "            values=values,\n",
    "            bins=edges,\n",
    "            bin_values=bin_values,\n",
    "            cut=cut_arr,\n",
    "            operator=op,\n",
    "        )\n",
    "    except Exception:\n",
    "        print(\"⚠ WARNING: evaluate_binned_cut failed — using manual selection fallback.\")\n",
    "        idx = np.digitize(bin_values, edges) - 1\n",
    "        sel = np.zeros_like(values, dtype=bool)\n",
    "        good = (idx >= 0) & (idx < len(cut_arr))\n",
    "        if op == \">=\":\n",
    "            sel[good] = values[good] >= cut_arr[idx[good]]\n",
    "        else:\n",
    "            sel[good] = values[good] <= cut_arr[idx[good]]\n",
    "        return sel\n",
    "\n",
    "\n",
    "# ---------------- gather files by zenith ----------------\n",
    "\n",
    "DL2 = Path(DL2_DIR)\n",
    "\n",
    "gamma_files       = sorted(glob.glob(str(DL2 / \"gamma_*.h5\")))\n",
    "gamma_point_files = sorted(glob.glob(str(DL2 / \"gamma_point_*.h5\")))\n",
    "proton_files      = sorted(glob.glob(str(DL2 / \"proton_*.h5\")))\n",
    "\n",
    "def group_by_zenith(paths):\n",
    "    d = {}\n",
    "    for p in paths:\n",
    "        z = zd_from_filename(p)\n",
    "        if z is None:\n",
    "            continue\n",
    "        d.setdefault(z, []).append(p)\n",
    "    return d\n",
    "\n",
    "G  = group_by_zenith(gamma_files)\n",
    "GP = group_by_zenith(gamma_point_files)\n",
    "P  = group_by_zenith(proton_files)\n",
    "\n",
    "zeniths = sorted(set(G.keys()) | set(GP.keys()) | set(P.keys()))\n",
    "if not zeniths:\n",
    "    raise RuntimeError(\"No DL2 files grouped by zenith found.\")\n",
    "\n",
    "\n",
    "# ---------------- build CSVs per efficiency, appending all zeniths ----------------\n",
    "\n",
    "OUT = Path(\"./final_CSV\")\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "eff_list = [0.40, 0.70, 0.90]\n",
    "\n",
    "for eff in eff_list:\n",
    "    gamma_csv = OUT / f\"SST1M_gamma_irf_gheffi_{eff:.2f}_theffi_{eff:.2f}.csv\"\n",
    "    backg_csv = OUT / f\"SST1M_backg_irf_gheffi_{eff:.2f}_theffi_{eff:.2f}.csv\"\n",
    "\n",
    "    # fresh start for this efficiency\n",
    "    if gamma_csv.exists():\n",
    "        gamma_csv.unlink()\n",
    "    if backg_csv.exists():\n",
    "        backg_csv.unlink()\n",
    "\n",
    "    for zd in zeniths:\n",
    "        g_paths = G.get(zd, []) + GP.get(zd, [])\n",
    "        p_paths = P.get(zd, [])\n",
    "\n",
    "        if not g_paths or not p_paths:\n",
    "            print(f\"[skip zd={zd}] gamma_paths={len(g_paths)} proton_paths={len(p_paths)}\")\n",
    "            continue\n",
    "\n",
    "        g_tab = stack_tables(g_paths)\n",
    "        p_tab = stack_tables(p_paths)\n",
    "        if g_tab is None or p_tab is None:\n",
    "            print(f\"[skip zd={zd}] could not stack tables\")\n",
    "            continue\n",
    "\n",
    "        # ------ extract basic arrays ------\n",
    "        g_true = to_value_tev(g_tab[\"true_energy\"])\n",
    "        g_reco = to_value_tev(g_tab[\"reco_energy\"])\n",
    "        g_gh   = np.asarray(g_tab[\"gammaness\"], float)\n",
    "        g_th   = np.rad2deg(\n",
    "            theta_from_altaz(\n",
    "                g_tab[\"true_alt\"], g_tab[\"true_az\"],\n",
    "                g_tab[\"reco_alt\"], g_tab[\"reco_az\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        p_true = to_value_tev(p_tab[\"true_energy\"])\n",
    "        p_reco = to_value_tev(p_tab[\"reco_energy\"])\n",
    "        p_gh   = np.asarray(p_tab[\"gammaness\"], float)\n",
    "        p_th   = np.rad2deg(\n",
    "            theta_from_altaz(\n",
    "                p_tab[\"true_alt\"], p_tab[\"true_az\"],\n",
    "                p_tab[\"reco_alt\"], p_tab[\"reco_az\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # ------ simulation info (gamma MC) ------\n",
    "        # Use only DL2 truth values, DO NOT use shower_distribution metadata\n",
    "        n_showers = len(g_true)   # simulated events\n",
    "\n",
    "        sim_g = SimulatedEventsInfo(\n",
    "            n_showers=n_showers,\n",
    "            energy_min=np.nanmin(g_true) * u.TeV,\n",
    "            energy_max=np.nanmax(g_true) * u.TeV,\n",
    "            max_impact=300.0 * u.m,        # SST-1M standard default\n",
    "            spectral_index=-2.0,\n",
    "            viewcone_min=0.0 * u.deg,\n",
    "            viewcone_max=0.0 * u.deg\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        # ------ binning in TRUE energy (for Aeff + migration) ------\n",
    "        true_bins = create_bins_per_decade(1 * u.TeV, 300.0 * u.TeV, bins_per_decade=10)\n",
    "        e_true_edges = true_bins.to_value(u.TeV)\n",
    "\n",
    "\n",
    "        # ------ binning in RECO energy (for background) ------\n",
    "        r_emin = max(0.05, float(np.nanpercentile(p_reco, 0.5)))\n",
    "        r_emax = min(80.0, float(np.nanpercentile(p_reco, 99.7)))\n",
    "        reco_bins = create_bins_per_decade((r_emin * u.TeV), (r_emax * u.TeV), bins_per_decade=8)\n",
    "        e_reco_edges = reco_bins.to_value(u.TeV)\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        # BACKGROUND: cuts vs RECO energy (for bkg spectrum)\n",
    "        # --------------------------------------------------\n",
    "        gh_cut_reco = percentile_cut_compat(\n",
    "            g_gh, g_reco, e_reco_edges,\n",
    "            eff_keep=eff,\n",
    "            upper_tail=True,     # keep upper tail in gammaness\n",
    "        )\n",
    "        th_cut_reco = percentile_cut_compat(\n",
    "            g_th, g_reco, e_reco_edges,\n",
    "            eff_keep=eff,\n",
    "            upper_tail=False,    # keep lower tail in theta\n",
    "        )\n",
    "\n",
    "        p_sel_gh = eval_binned(p_gh, p_reco, e_reco_edges, gh_cut_reco, op=\">=\")\n",
    "        p_sel_th = eval_binned(p_th, p_reco, e_reco_edges, th_cut_reco, op=\"<=\")\n",
    "        p_sel = p_sel_gh & p_sel_th\n",
    "\n",
    "        counts_sel, _ = np.histogram(p_reco[p_sel], bins=e_reco_edges)\n",
    "\n",
    "        # total background rate (normalization knob)\n",
    "        total_rate_hz = 20.0\n",
    "        if counts_sel.sum() == 0:\n",
    "            rates = np.full(len(e_reco_edges) - 1, 1e-12)\n",
    "            print(f\"⚠ WARNING: No selected protons at zd={zd}, eff={eff:.2f} — setting tiny background.\")\n",
    "        else:\n",
    "            rates = total_rate_hz * counts_sel.astype(float) / counts_sel.sum()\n",
    "            rates = np.maximum(rates, 1e-12)\n",
    "\n",
    "        df_back = pd.DataFrame({\n",
    "            \"ZD_deg\": np.full(len(e_reco_edges) - 1, zd, dtype=float),\n",
    "            \"Ereco_min_TeV\": e_reco_edges[:-1].astype(float),\n",
    "            \"Ereco_max_TeV\": e_reco_edges[1:].astype(float),\n",
    "            \"BckgRate_per_second\": rates.astype(float),\n",
    "            \"Theta_cut_deg\": th_cut_reco.astype(float),\n",
    "            \"Gammaness_cut\": gh_cut_reco.astype(float),\n",
    "        })\n",
    "\n",
    "        df_back.to_csv(backg_csv, mode=\"a\", header=not backg_csv.exists(), index=False)\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        # GAMMA: cuts vs TRUE energy (for Aeff & migration)\n",
    "        # --------------------------------------------------\n",
    "        gh_cut_true = percentile_cut_compat(\n",
    "            g_gh, g_true, e_true_edges,\n",
    "            eff_keep=eff,\n",
    "            upper_tail=True,\n",
    "        )\n",
    "        th_cut_true = percentile_cut_compat(\n",
    "            g_th, g_true, e_true_edges,\n",
    "            eff_keep=eff,\n",
    "            upper_tail=False,\n",
    "        )\n",
    "\n",
    "        g_sel_gh = eval_binned(g_gh, g_true, e_true_edges, gh_cut_true, op=\">=\")\n",
    "        g_sel_th = eval_binned(g_th, g_true, e_true_edges, th_cut_true, op=\"<=\")\n",
    "        g_sel = g_sel_gh & g_sel_th\n",
    "\n",
    "        t_selected = Table()\n",
    "        t_selected[\"true_energy\"] = (g_true[g_sel] * u.TeV)\n",
    "        aeff_sel = effective_area_per_energy(t_selected, sim_g, true_bins).to_value(u.m**2)\n",
    "\n",
    "        # --------------------------------------------\n",
    "        # Energy-migration fit: ratio Ereco/Etrue\n",
    "        # --------------------------------------------\n",
    "        n_true_bins = len(e_true_edges) - 1\n",
    "        mu_loc   = np.full(n_true_bins, np.nan)\n",
    "        mu_scale = np.full(n_true_bins, np.nan)\n",
    "        mu_a     = np.full(n_true_bins, np.nan)\n",
    "        model    = np.array([\"moyal\"] * n_true_bins, dtype=object)\n",
    "\n",
    "        for i in range(n_true_bins):\n",
    "            m = g_sel & (g_true >= e_true_edges[i]) & (g_true < e_true_edges[i + 1])\n",
    "            if np.count_nonzero(m) < 20:\n",
    "                continue\n",
    "\n",
    "            ratio = g_reco[m] / np.clip(g_true[m], 1e-20, None)\n",
    "            ratio = np.clip(ratio, 1e-6, 1e6)\n",
    "\n",
    "            med = float(np.nanmedian(ratio))\n",
    "            mad = float(np.nanmedian(np.abs(ratio - med)))\n",
    "            sigma = max(1.4826 * mad, 1e-3)\n",
    "            clean = ratio[np.abs(ratio - med) < 3.0 * sigma]\n",
    "            if clean.size < 20:\n",
    "                clean = ratio\n",
    "\n",
    "            # moyal fit\n",
    "            try:\n",
    "                loc_m, scale_m = moyal.fit(clean, loc=med, scale=sigma)\n",
    "            except Exception:\n",
    "                loc_m, scale_m = med, sigma\n",
    "\n",
    "            ok_m = (\n",
    "                np.isfinite(loc_m) and np.isfinite(scale_m)\n",
    "                and (0.01 < scale_m < 3.0)\n",
    "            )\n",
    "\n",
    "            # skewnorm fit\n",
    "            try:\n",
    "                a_s, loc_s, scale_s = skewnorm.fit(clean, loc=med, scale=sigma)\n",
    "                ok_s = (\n",
    "                    np.isfinite(a_s) and np.isfinite(loc_s) and np.isfinite(scale_s)\n",
    "                    and abs(a_s) < 30.0\n",
    "                    and (0.01 < scale_s < 3.0)\n",
    "                )\n",
    "            except Exception:\n",
    "                ok_s = False\n",
    "                a_s = loc_s = scale_s = np.nan\n",
    "\n",
    "            if ok_s and (abs(a_s) > 0.5):\n",
    "                mu_loc[i], mu_scale[i], mu_a[i] = float(loc_s), float(scale_s), float(a_s)\n",
    "                model[i] = \"skewnorm\"\n",
    "            elif ok_m:\n",
    "                mu_loc[i], mu_scale[i], mu_a[i] = float(loc_m), float(scale_m), np.nan\n",
    "                model[i] = \"moyal\"\n",
    "            else:\n",
    "                mu_loc[i], mu_scale[i], mu_a[i] = med, sigma, np.nan\n",
    "                model[i] = \"moyal\"\n",
    "\n",
    "        df_gamma = pd.DataFrame({\n",
    "            \"ZD_deg\": np.full(n_true_bins, zd, dtype=float),\n",
    "            \"Etrue_min_TeV\": e_true_edges[:-1].astype(float),\n",
    "            \"Etrue_max_TeV\": e_true_edges[1:].astype(float),\n",
    "            \"Aeff_m2\": aeff_sel.astype(float),\n",
    "            \"emig_mu_loc\": mu_loc.astype(float),\n",
    "            \"emig_mu_scale\": mu_scale.astype(float),\n",
    "            \"emig_mu_a\": mu_a.astype(float),\n",
    "            \"emig_model\": model,\n",
    "        })\n",
    "\n",
    "        df_gamma.to_csv(gamma_csv, mode=\"a\", header=not gamma_csv.exists(), index=False)\n",
    "\n",
    "        print(f\"[zd={zd:>2.0f} eff={eff:.2f}] rows: back={len(df_back)}, gamma={len(df_gamma)}\")\n",
    "\n",
    "print(\"Done. CSVs in:\", OUT.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3212dfd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aeff_sel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43maeff_sel\u001b[49m):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(i, e_true_edges[i], a)\n",
      "\u001b[31mNameError\u001b[39m: name 'aeff_sel' is not defined"
     ]
    }
   ],
   "source": [
    "for i, a in enumerate(aeff_sel):\n",
    "    print(i, e_true_edges[i], a)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
